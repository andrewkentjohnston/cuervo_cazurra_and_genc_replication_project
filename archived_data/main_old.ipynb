{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import LDCs greenfield FDI info for years 2017-2018. List of LDCs taken from UNCTAD (2017, 2018). Greenfield obtained from FT fDi Markets database and captures all transactions over USD 500,000. 2017_2018_greenfield_data_fdi_markets_original.csv saved as \"main_dataset_greenfield.csv\" and all variables and all country names converted to snake_case to faciliate data wrangling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './raw_data/2017_2018_greenfield_data_fdi_markets_original.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load data from fDi Markets\u001b[39;00m\n\u001b[1;32m      4\u001b[0m fdi_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./raw_data/2017_2018_greenfield_data_fdi_markets_original.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfdi_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Define function to convert text to snake_case\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_snake_case\u001b[39m(s):\n",
      "File \u001b[0;32m~/coding_projects/nov_2024_replication_project/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coding_projects/nov_2024_replication_project/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/coding_projects/nov_2024_replication_project/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coding_projects/nov_2024_replication_project/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/coding_projects/nov_2024_replication_project/.venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './raw_data/2017_2018_greenfield_data_fdi_markets_original.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from fDi Markets\n",
    "fdi_data = \"./raw_data/2017_2018_greenfield_data_fdi_markets_original.csv\"\n",
    "df = pd.read_csv(fdi_data)\n",
    "\n",
    "# Define function to convert text to snake_case\n",
    "def to_snake_case(s):\n",
    "    return s.strip().replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n",
    "\n",
    "# Convert variable names (i.e., column headers) to snake_case for data wrangling\n",
    "df.columns = [to_snake_case(col) for col in df.columns]\n",
    "\n",
    "# Convert values in 'destination_country\" and \"source_country\" to snake_case\n",
    "df[\"destination_country\"] = df[\"destination_country\"].apply(to_snake_case)\n",
    "df[\"source_country\"] = df[\"source_country\"].apply(to_snake_case)\n",
    "\n",
    "# Save modified DataFrame to new CSV file\n",
    "output_filepath = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "df.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"File saved to {output_filepath}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create spreadsheet of unique destination countries for data auditing/validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to ./audit_data/unique_destinationa_countries.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afghanistan</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angola</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bangladesh</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>benin</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bhutan</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>burkina_faso</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>burundi</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cambodia</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>central_african_republic</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chad</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>democratic_republic_of_congo</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>djibouti</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>east_timor</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ethiopia</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gambia</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>guinea</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>haiti</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>laos</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lesotho</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>liberia</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>madagascar</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>malawi</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mali</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>mauritania</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mozambique</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>myanmar</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>nepal</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>niger</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rwanda</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>senegal</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>sierra_leone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>somalia</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>sudan</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>tanzania</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>togo</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>uganda</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>zambia</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         country  count\n",
       "0                    afghanistan      3\n",
       "1                         angola     11\n",
       "2                     bangladesh     47\n",
       "3                          benin      8\n",
       "4                         bhutan      6\n",
       "5                   burkina_faso      8\n",
       "6                        burundi      4\n",
       "7                       cambodia     92\n",
       "8       central_african_republic      2\n",
       "9                           chad      2\n",
       "10  democratic_republic_of_congo     15\n",
       "11                      djibouti      3\n",
       "12                    east_timor      2\n",
       "13                      ethiopia     54\n",
       "14                        gambia      4\n",
       "15                        guinea     13\n",
       "16                         haiti      3\n",
       "17                          laos     25\n",
       "18                       lesotho      5\n",
       "19                       liberia      8\n",
       "20                    madagascar      5\n",
       "21                        malawi      7\n",
       "22                          mali      5\n",
       "23                    mauritania      2\n",
       "24                    mozambique     45\n",
       "25                       myanmar    117\n",
       "26                         nepal     15\n",
       "27                         niger      2\n",
       "28                        rwanda     17\n",
       "29                       senegal     20\n",
       "30                  sierra_leone      1\n",
       "31                       somalia      3\n",
       "32                         sudan      3\n",
       "33                      tanzania     42\n",
       "34                          togo      3\n",
       "35                        uganda     34\n",
       "36                        zambia     31"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load processed greenfield dataset\n",
    "filepath = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# Get unique country names and count of each\n",
    "unique_destination_countries = df[\"destination_country\"].value_counts().reset_index()\n",
    "\n",
    "# Rename columns\n",
    "unique_destination_countries.columns = [\"country\", \"count\"]\n",
    "\n",
    "# Sort new df alphabetically\n",
    "unique_destination_countries = unique_destination_countries.sort_values(by=\"country\").reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_filepath = \"./audit_data/unique_destinationa_countries.csv\"\n",
    "unique_destination_countries.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"File saved to {output_filepath}\")\n",
    "\n",
    "# Display results\n",
    "unique_destination_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare list of destination countries with UNCTAD 2017-2018 LDCs list for data audit/validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDCs not appearing as desintation countries in greenfield_main_dataset.csv:\n",
      "comoros\n",
      "eritrea\n",
      "guinea_bissau\n",
      "kiribati\n",
      "sao_tome_and_principe\n",
      "solomon_islands\n",
      "south_sudan\n",
      "tuvalu\n",
      "vanuatu\n",
      "yemen\n",
      "Countries appearing as destination countires in greenfield_main_dataset.csv but not appearing in LDCs list:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both spreadsheets into DataFrames\n",
    "destination_countries_df = pd.read_csv(\"./audit_data/unique_destinationa_countries.csv\")\n",
    "ldcs_df = pd.read_csv(\"./variable_data/ldcs_list_2017_2018.csv\")\n",
    "\n",
    "# Convert country names to snake_case\n",
    "def to_snake_case(s):\n",
    "    return s.strip().replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n",
    "\n",
    "destination_countries_df[\"country\"] = destination_countries_df[\"country\"].apply(to_snake_case)\n",
    "ldcs_df[\"country\"] = ldcs_df[\"country\"].apply(to_snake_case)\n",
    "\n",
    "# Convert pandas Series objects into sets for comparison\n",
    "destination_countries_set = set(destination_countries_df[\"country\"])\n",
    "ldcs_set = set(ldcs_df[\"country\"])\n",
    "\n",
    "# Create new sets of missing countries\n",
    "missing_in_destination_countries_list = sorted(ldcs_set - destination_countries_set)\n",
    "missing_in_ldcs_list = sorted(destination_countries_set - ldcs_set)\n",
    "\n",
    "# Display results\n",
    "print(\"LDCs not appearing as desintation countries in greenfield_main_dataset.csv:\")\n",
    "for country in missing_in_destination_countries_list:\n",
    "    print(country)\n",
    "print(\"Countries appearing as destination countires in greenfield_main_dataset.csv but not appearing in LDCs list:\")\n",
    "for country in missing_in_ldcs_list:\n",
    "    print(country)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDCs not appearing in greenfield FDI are consistent with those for which there was no data in fDi Markets for those years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create spreadsheet of unique source countries for data auditing/validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to ./audit_data/unique_source_countries.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>australia</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>austria</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>azerbaijan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>barbados</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>belgium</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>uae</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ukraine</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>united_kingdom</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>united_states</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>vietnam</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           country  count\n",
       "0        australia      4\n",
       "1          austria      2\n",
       "2       azerbaijan      1\n",
       "3         barbados      1\n",
       "4          belgium      6\n",
       "..             ...    ...\n",
       "58             uae     26\n",
       "59         ukraine      4\n",
       "60  united_kingdom     28\n",
       "61   united_states     49\n",
       "62         vietnam     23\n",
       "\n",
       "[63 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load processed greenfield dataset\n",
    "filepath = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# Get unique source country names and count of each\n",
    "unique_source_countries = df[\"source_country\"].value_counts().reset_index()\n",
    "\n",
    "# Rename columns\n",
    "unique_source_countries.columns = [\"country\", \"count\"]\n",
    "\n",
    "# Sort new df alphabetically\n",
    "unique_source_countries = unique_source_countries.sort_values(by=\"country\").reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_filepath = \"./audit_data/unique_source_countries.csv\"\n",
    "unique_source_countries.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"File saved to {output_filepath}\")\n",
    "\n",
    "# Display results\n",
    "unique_source_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare list of source countries with UNCTAD 2017-2018 developed country list for data audit/validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developed countries not appearing as source countries in greenfield_main_dataset.csv:\n",
      "andorra\n",
      "bermuda\n",
      "bulgaria\n",
      "czechia\n",
      "estonia\n",
      "faroe_islands\n",
      "gibraltar\n",
      "greece\n",
      "greenland\n",
      "holy_see\n",
      "hungary\n",
      "iceland\n",
      "ireland\n",
      "latvia\n",
      "lithuania\n",
      "luxembourg\n",
      "malta\n",
      "poland\n",
      "romania\n",
      "saint_pierre_and_miquelon\n",
      "san_marino\n",
      "slovakia\n",
      "slovenia\n",
      "sweden\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both spreadsheets into DataFrames\n",
    "source_countries_df = pd.read_csv(\"./audit_data/unique_source_countries.csv\")\n",
    "developed_countries_df = pd.read_csv(\"./variable_data/developed_countries_2017_2018.csv\")\n",
    "\n",
    "\n",
    "# Convert country names to snake_case\n",
    "def to_snake_case(s):\n",
    "    return s.strip().replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n",
    "\n",
    "\n",
    "source_countries_df[\"country\"] = source_countries_df[\"country\"].apply(\n",
    "    to_snake_case\n",
    ")\n",
    "developed_countries_df[\"country\"] = developed_countries_df[\"country\"].apply(to_snake_case)\n",
    "\n",
    "# Convert pandas Series objects into sets for comparison\n",
    "source_countries_set = set(source_countries_df[\"country\"])\n",
    "developed_countries_set = set(developed_countries_df[\"country\"])\n",
    "\n",
    "# Create new sets of missing countries\n",
    "missing_in_source_countries = developed_countries_set - source_countries_set\n",
    "\n",
    "# Display results\n",
    "print(\"Developed countries not appearing as source countries in greenfield_main_dataset.csv:\")\n",
    "for country in sorted(missing_in_source_countries):\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developed countries not appearing in greenfield_main_dataset were checked manually against unique_source_countries.csv to ensure no matches were missed due to spelling or punctuation issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"developed\" variable in greenfield_main_dataset.csv. Variable takes value of True if source country appears in list of developed countries and False otherwise. Also created \"emne\" variable that takes the inverse value of the \"developed\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greenfield_main_dataset.csv updated with \"developed\" and \"emne\" variables.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load spreadsheets into DataFrames\n",
    "\n",
    "greenfield_filepath = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "developed_countries_filepath = \"./variable_data/developed_countries_2017_2018.csv\"\n",
    "\n",
    "greenfield_df = pd.read_csv(greenfield_filepath)\n",
    "developed_countries_df = pd.read_csv(developed_countries_filepath)\n",
    "\n",
    "# Create set of developed countries\n",
    "developed_countries_set = set(developed_countries_df[\"country\"])\n",
    "\n",
    "# Add \"developed\" variable to greenfield_df\n",
    "greenfield_df[\"developed\"] = greenfield_df[\"source_country\"].isin(developed_countries_set)\n",
    "\n",
    "# Add \"enne\" variable to greenfield_df\n",
    "greenfield_df[\"emne\"] = ~greenfield_df[\"source_country\"].isin(developed_countries_set)\n",
    "\n",
    "# Save the updated DataFrame as greenfield_main_dataset.csv \n",
    "greenfield_df.to_csv(greenfield_filepath, index=False)\n",
    "\n",
    "print(f\"greenfield_main_dataset.csv updated with \\\"developed\\\" and \\\"emne\\\" variables.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"bordering_country\" variable. Variable is True if source countries shares a border with destination country and False otherwise. Bordering countries checked from CIA World Factbook https://www.cia.gov/the-world-factbook/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new variable \"bordering country\" has been added and the updated file has been saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load spreadsheets into DataFrames\n",
    "greenfield_filepath = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "bordering_countries_filepath = \"./variable_data/bordering_countries_2017_2018.csv\"\n",
    "\n",
    "greenfield_df = pd.read_csv(greenfield_filepath)\n",
    "bordering_countries_df = pd.read_csv(bordering_countries_filepath)\n",
    "\n",
    "# Create dictionary of bordering countries\n",
    "bordering_countries_dict = bordering_countries_df.set_index(\"ldc\")[\"bordering_countries\"].to_dict()\n",
    "\n",
    "# Function to check if the source country is in the list of bordering countries\n",
    "def is_bordering(source_country, destination_country):\n",
    "    bordering_countries = bordering_countries_dict.get(destination_country, \"\")\n",
    "    if pd.isna(bordering_countries) or not bordering_countries:\n",
    "        return False\n",
    "    bordering_list = bordering_countries.split(\", \")\n",
    "    return source_country in bordering_list\n",
    "\n",
    "# Apply the is_bordering function to create the new \"bordering_country\" variable in greenfield_df\n",
    "greenfield_df[\"bordering_country\"] = greenfield_df.apply(\n",
    "    lambda row: is_bordering(row[\"source_country\"], row[\"destination_country\"]), axis=1\n",
    ")\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "greenfield_df.to_csv(greenfield_filepath, index=False)\n",
    "\n",
    "print(f\"The new variable \\\"bordering country\\\" has been added and the updated file has been saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new \"industry\" variable by joining \"sector\", \"sub_sector\", and \"activity\". Create list/spreadsheet of all unique values in \"industry\" column (and their count) from greenfield_main_dataset.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset has been saved to ./processed_data/greenfield_main_dataset.csv.\n",
      "        Industry counts has been saved to ./audit_data/industry_counts.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load processed greenfield dataset\n",
    "filepath = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# Define function to convert strings to snake_case\n",
    "def to_snake_case(s):\n",
    "    s = (\n",
    "        s.replace(\" \", \"_\")\n",
    "        .replace(\"&\", \"and\")\n",
    "        .replace(\",\", \"\")\n",
    "        .replace(\"/\", \"\")\n",
    "        .replace(\"-\", \"_\")\n",
    "        .replace(\",\", \"\")\n",
    "        .replace(\"(\", \"\")\n",
    "        .replace(\")\", \"\")\n",
    "    )\n",
    "    return s.lower()\n",
    "\n",
    "# Create a new 'industry' column by joining 'sector', 'sub-sector', and 'activity'\n",
    "df[\"industry\"] = (\n",
    "    df[[\"sector\", \"sub_sector\", \"activity\"]]\n",
    "    .fillna(\"\")\n",
    "    .apply(lambda x: \" \".join(x), axis=1)\n",
    "    \n",
    ")\n",
    "\n",
    "# Convert the new 'industry' column to snake_case\n",
    "df[\"industry\"] = df[\"industry\"].apply(to_snake_case)\n",
    "\n",
    "# Save the modified DataFrame to CSV\n",
    "df.to_csv(filepath, index=False)\n",
    "\n",
    "# Calculate the count of unique values in 'industry' column \n",
    "industry_counts = df[\"industry\"].value_counts().reset_index()\n",
    "industry_counts.columns = [\"industry\", \"count\"]\n",
    "\n",
    "# Sort alphabetically\n",
    "industry_counts.sort_values(by=\"industry\", inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "industry_counts_filepath = \"./audit_data/industry_counts.csv\"\n",
    "industry_counts.to_csv(industry_counts_filepath, index=False)\n",
    "\n",
    "print(\n",
    "    f\"Updated dataset has been saved to {filepath}.\\n\\\n",
    "        Industry counts has been saved to {industry_counts_filepath}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"natural_resource\" variable. Variable is True if industry is listed as a natural resource industry in the \"nat_resource_list.csv\" spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The modified dataset has been saved to ./processed_data/greenfield_main_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both spreadsheets into DataFrames\n",
    "main_dataset_path = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "nat_resource_list_path = \"./variable_data/nat_resource_list.csv\"\n",
    "\n",
    "main_df = pd.read_csv(main_dataset_path)\n",
    "nat_resource_df = pd.read_csv(nat_resource_list_path)\n",
    "\n",
    "# Create a set of natural resource industries\n",
    "natural_resource_set = set(\n",
    "    nat_resource_df[nat_resource_df[\"natural_resource\"] == True][\"industry\"]\n",
    ")\n",
    "\n",
    "# Create a new column \"natural_resource_ind\" in main_df\n",
    "main_df[\"natural_resource_ind\"] = main_df[\"industry\"].apply(\n",
    "    lambda x: x in natural_resource_set\n",
    ")\n",
    "\n",
    "# Save the modified DataFrame to CSV\n",
    "main_df.to_csv(main_dataset_path, index=False)\n",
    "\n",
    "print(\n",
    "    f\"The modified dataset has been saved to {main_dataset_path}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"colonial_link\" variable. Variable is True if the source country is listed in the \"colonial_rulers\" column for that country in \"colonial_rulers.csv\" and False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset saved to ./processed_data/greenfield_main_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both spreadsheets into DataFrames\n",
    "main_dataset_path = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "colonial_rulers_path = \"./variable_data/colonial_rulers_list.csv\"\n",
    "\n",
    "main_df = pd.read_csv(main_dataset_path)\n",
    "colonial_rulers_df = pd.read_csv(colonial_rulers_path)\n",
    "\n",
    "# Create a dictionary from colonial_rulers_list\n",
    "colonial_rulers_dict = colonial_rulers_df.set_index(\"country\")[\n",
    "    \"colonial_rulers\"\n",
    "    ].to_dict()\n",
    "\n",
    "# Function to check for colonial link\n",
    "def has_colonial_link(row):\n",
    "    destination = row[\"destination_country\"]\n",
    "    source = row[\"source_country\"]\n",
    "    colonial_rulers = colonial_rulers_dict.get(destination)\n",
    "\n",
    "    if isinstance(colonial_rulers, str):\n",
    "        colonial_rulers_list = [\n",
    "            ruler.strip().replace(\" \", \"_\").replace(\"-\", \"_\").lower() for ruler in colonial_rulers.split(\", \")\n",
    "        ]\n",
    "        return source in colonial_rulers_list\n",
    "    return False\n",
    "\n",
    "# Apply the function to create new \"colonial_link\" column\n",
    "main_df[\"colonial_link\"] = main_df.apply(has_colonial_link, axis=1)\n",
    "\n",
    "# Save the updated DatafRame to CSV\n",
    "main_df.to_csv(main_dataset_path, index=False)\n",
    "\n",
    "print(f\"Updated dataset saved to {main_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 2017 and 2018 spreadsheets for each individual LDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country spreadsheets created and saved to \"country_spreadsheets/greenfield\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load both spreadsheets into DataFrames\n",
    "main_dataset_path = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "ldcs_list_path = \"./variable_data/ldcs_list_2017_2018.csv\"\n",
    "\n",
    "main_df = pd.read_csv(main_dataset_path)\n",
    "ldcs_df = pd.read_csv(ldcs_list_path)\n",
    "\n",
    "\n",
    "# Define function to convert country names to snake_case\n",
    "def to_snake_case(s):\n",
    "    return s.strip().replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n",
    "\n",
    "# Convert country names in both DataFrames to snake_case\n",
    "main_df[\"destination_country\"].apply(to_snake_case)\n",
    "ldcs_df[\"country\"].apply(to_snake_case)\n",
    "\n",
    "# Convert \"project_date\" to datetime for easier filtering\n",
    "main_df[\"project_date\"] = pd.to_datetime(\n",
    "    main_df[\"project_date\"], format=\"%b %Y\"\n",
    ")\n",
    "\n",
    "# Extract year from \"project_date\"\n",
    "main_df[\"year\"] = main_df[\"project_date\"].dt.year\n",
    "\n",
    "# Create output directories for country spreadsheets\n",
    "output_dir = \"./country_spreadsheets/greenfield\"\n",
    "\n",
    "for year in [2017, 2018]:\n",
    "    year_dir = os.path.join(output_dir, str(year))\n",
    "    if not os.path.exists(year_dir):\n",
    "        os.makedirs(year_dir)\n",
    "\n",
    "# Create list to keep track of rows not assigned to a spreadhseet\n",
    "unassigned_rows = []\n",
    "\n",
    "# List to keep track of empty spreadsheets\n",
    "empty_spreadsheets = []\n",
    "\n",
    "# Proess each country in the ldcs_df\n",
    "for country in ldcs_df[\"country\"]:\n",
    "    for year in [2017, 2018]:\n",
    "        # Filter data for the specific country and year\n",
    "        country_year_data = main_df[\n",
    "            (main_df[\"destination_country\"] == country)\n",
    "            & (main_df[\"year\"] == year)\n",
    "        ]\n",
    "\n",
    "        # Create filename\n",
    "        year_dir = os.path.join(output_dir, str(year))\n",
    "        filename = f\"{year_dir}/{country}_greenfield_{year}.csv\"\n",
    "\n",
    "        # Save data to CSV\n",
    "        if not country_year_data.empty:\n",
    "            country_year_data.to_csv(filename, index=False)\n",
    "        else:\n",
    "            # Save an empty CSV with the same headers as greenfield_main_dataset.csv\n",
    "            empty_df = main_df.head(0)\n",
    "            empty_df.to_csv(filename, index=False)\n",
    "            empty_spreadsheets.append(filename)\n",
    "\n",
    "# Identify rows not assigned to any country spreadsheet\n",
    "assigned_rows = main_df[\n",
    "    main_df[\"destination_country\"].isin(ldcs_df[\"country\"])\n",
    "]\n",
    "unassigned_rows = main_df[~main_df.index.isin(assigned_rows.index)]\n",
    "\n",
    "# Save unassigned rows to a CSV file\n",
    "audit_dir = \"./audit_data\"\n",
    "unassigned_filename = f\"{audit_dir}/unassigned_rows.csv\"\n",
    "unassigned_rows.to_csv(unassigned_filename, index=False)\n",
    "\n",
    "# Save list of empty spreadsheets to a CSV file\n",
    "empty_spreadsheets_filename = f\"{audit_dir}/empty_spreadsheets.csv\"\n",
    "pd.DataFrame(empty_spreadsheets, columns=[\"empty_spreadsheets\"]).to_csv(\n",
    "    empty_spreadsheets_filename, index=False\n",
    ")\n",
    "\n",
    "print(\"Country spreadsheets created and saved to \\\"country_spreadsheets/greenfield\\\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create master spreadsheets for 2017 and 2018 that show greenfield FDI total; total excluding natural resource industries, and total excluding firms from former colonial powers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master spreadsheets created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory paths\n",
    "base_dir = \"./country_spreadsheets\"\n",
    "years = [\"2017\", \"2018\"]\n",
    "output_dir = \"./processed_data\"\n",
    "\n",
    "# Initialize an empty list for each year to hold data\n",
    "data_2017 = []\n",
    "data_2018 = []\n",
    "\n",
    "# Function to process each file and extract the required information\n",
    "def process_file(filepath, country, year):\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    total_greenfield = df[\"capital_investment\"].sum()\n",
    "    total_greenfield_excl_nat_res = df[df[\"natural_resource_ind\"] == False][\n",
    "        \"capital_investment\"\n",
    "    ].sum()\n",
    "    total_greenfield_excl_col_link = df[df[\"colonial_link\"] == False][\n",
    "        \"capital_investment\"\n",
    "    ].sum()\n",
    "    emne_greenfield = df[df[\"emne\"] == True][\"capital_investment\"].sum()\n",
    "    emne_greenfield_exl_nat_res = df[\n",
    "        (df[\"emne\"] == True) & (df[\"natural_resource_ind\"] == False)\n",
    "    ][\"capital_investment\"].sum()\n",
    "    emne_greenfield_exl_col_link = df[\n",
    "        (df[\"emne\"] == True) & (df[\"colonial_link\"] == False)\n",
    "    ][\"capital_investment\"].sum()\n",
    "\n",
    "    return {\n",
    "        \"country\": country,\n",
    "        \"year\": year,\n",
    "        \"total_greenfield\": total_greenfield,\n",
    "        \"total_greenefield_excl_nat_res\": total_greenfield_excl_nat_res,\n",
    "        \"total_greenfield_excl_col_link\": total_greenfield_excl_col_link,\n",
    "        \"emne_greenfield\": emne_greenfield,\n",
    "        \"emne_greenfield_excl_nat_res\": emne_greenfield_exl_nat_res,\n",
    "        \"emne_greenfield_excl_col_link\": emne_greenfield_exl_col_link,\n",
    "    }\n",
    "\n",
    "# Loop through each year and process the files\n",
    "for year in years:\n",
    "    year_dir = os.path.join(base_dir, \"greenfield\", year)\n",
    "    for file_name in os.listdir(year_dir):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            parts = file_name.split(\"_\")\n",
    "\n",
    "            # Extract country name by joining all parts except the last two\n",
    "            country = \"_\".join(parts[:-2]) \n",
    "            filepath = os.path.join(year_dir, file_name)\n",
    "            data = process_file(filepath, country, year)\n",
    "            if year == \"2017\":\n",
    "                data_2017.append(data)\n",
    "            else:\n",
    "                data_2018.append(data)\n",
    "\n",
    "# Create DataFrames for 2017 and 2018\n",
    "df_2017 = pd.DataFrame(data_2017)\n",
    "df_2018 = pd.DataFrame(data_2018)\n",
    "\n",
    "# Sort the DataFrames\n",
    "df_2017 = df_2017.sort_values(by=[\"country\"])\n",
    "df_2018 = df_2018.sort_values(by=[\"country\"])\n",
    "\n",
    "# Save the DataFrames to CSV\n",
    "df_2017.to_csv(os.path.join(output_dir, \"master_dataset_2017.csv\"), index=False)\n",
    "df_2018.to_csv(os.path.join(output_dir, \"master_dataset_2018.csv\"), index=False)\n",
    "\n",
    "print(\"Master spreadsheets created successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add governance variables/data from World Governance Indicators spreadsheet found here: https://www.worldbank.org/content/dam/sites/govindicators/doc/wgidataset.xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize names to same format as main datasets for data wrangling. Names checked manually and presented in 'standardized' format in \"./wgi_data/supplementary_data/name_conversions_wgi.csv\". \n",
    "\n",
    "Filter WGI variables down to LDCs and check for any missing LDC countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All LDCs were found in every file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load name conversion and ldcs spreadsheets\n",
    "name_conversions = pd.read_csv(\"./variable_data/wgi_data/supplementary_data/name_conversions_wgi.csv\")\n",
    "ldcs_df = pd.read_csv(\"./variable_data/ldcs_list_2017_2018.csv\")\n",
    "\n",
    "# Create a dictionary from the name conversions, dropping NaN values\n",
    "name_conversions_dict = {\n",
    "    row[\"old_name\"]: row[\"new_name\"] for _, row in name_conversions.dropna().iterrows()\n",
    "}\n",
    "\n",
    "# Dictionary of WGI variable filepaths\n",
    "filepaths = {\n",
    "    \"./raw_data/wgi_raw_data/voice_and_acc.csv\": \"variable_data/wgi_data/wgi_variables/voice_and_acc.csv\",\n",
    "    \"./raw_data/wgi_raw_data/pol_stability.csv\": \"variable_data/wgi_data/wgi_variables/pol_stability.csv\",\n",
    "    \"./raw_data/wgi_raw_data/govt_effectiveness.csv\": \"variable_data/wgi_data/wgi_variables/govt_effectiveness.csv\",\n",
    "    \"./raw_data/wgi_raw_data/reg_quality.csv\": \"variable_data/wgi_data/wgi_variables/reg_quality.csv\",\n",
    "    \"./raw_data/wgi_raw_data/rule_of_law.csv\": \"variable_data/wgi_data/wgi_variables/rule_of_law.csv\",\n",
    "    \"./raw_data/wgi_raw_data/control_of_corruption.csv\": \"variable_data/wgi_data/wgi_variables/control_of_corruption.csv\",\n",
    "}\n",
    "\n",
    "# Define function to convert names to snake_case\n",
    "def to_snake_case(s):\n",
    "    return s.strip().replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n",
    "\n",
    "# Define function to filter countries and check for missing LDCs\n",
    "def filter_and_check(df, ldcs_set):\n",
    "    filtered_df = df[df[\"country\"].isin(ldcs_set)]\n",
    "    found_countries = set(filtered_df[\"country\"])\n",
    "    missing_countries = ldcs_set - found_countries\n",
    "    return filtered_df, missing_countries\n",
    "\n",
    "# Function to convert country names and save to new file\n",
    "def convert_country_names(input_path, output_path, name_conversions_dict):\n",
    "    df = pd.read_csv(input_path)\n",
    "    # Convert cocuntry names using the dictionary and apply snake_case conversion\n",
    "    df[\"country\"] = df[\"country\"].apply(lambda x: to_snake_case(name_conversions_dict.get(x, x)))\n",
    "    df.to_csv(output_path, index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Convert country names to snake_case in the LDCs DataFrame\n",
    "ldcs_df[\"country\"] = ldcs_df[\"country\"].apply(to_snake_case)\n",
    "\n",
    "# Create LDCs set\n",
    "ldcs_set = set(ldcs_df[\"country\"])\n",
    "\n",
    "# Dictionary to store missing countries for each file\n",
    "missing_countries_dict = {}\n",
    "\n",
    "# Apply the conversion to each file in \"raw_data/wgi_raw_data\" and save to \"variable_data/wgi_data/wgi_variables\"\n",
    "for input_path, output_path, in filepaths.items():\n",
    "    df = convert_country_names(input_path, output_path, name_conversions_dict)\n",
    "    filtered_df, missing_countries = filter_and_check(df, ldcs_set)\n",
    "    if missing_countries:\n",
    "        missing_countries_dict[output_path] = list(missing_countries)\n",
    "    filtered_df.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "# Check if there are any missing countries\n",
    "if missing_countries_dict:\n",
    "    # Convert the missing countries dictionary to a DataFrame and print\n",
    "    missing_countries_df = pd.DataFrame(\n",
    "        {k: pd.Series(v) for k, v in missing_countries_dict.items()}\n",
    "    )\n",
    "    print(\"Missing Countries in Each File:\")\n",
    "    print(missing_countries_df)\n",
    "else:\n",
    "    # All LDCs were found in every file\n",
    "    print(\"All LDCs were found in every file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add WGI variables to master datasets (2017 and 2018).\n",
    "2016 year data added to \"./processed_data/master_dataset_2017.csv\"\n",
    "2017 year data added to \"./processed_data/master_dataset_2018.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets have been updated and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the master datasets\n",
    "master_datasets = {\n",
    "    2017: pd.read_csv(\"./processed_data/master_dataset_2017.csv\"),\n",
    "    2018: pd.read_csv(\"./processed_data/master_dataset_2018.csv\"),\n",
    "}\n",
    "\n",
    "# List of WGI files\n",
    "wgi_files = [\n",
    "    \"./variable_data/wgi_data/wgi_variables/voice_and_acc.csv\",\n",
    "    \"./variable_data/wgi_data/wgi_variables/pol_stability.csv\",\n",
    "    \"./variable_data/wgi_data/wgi_variables/govt_effectiveness.csv\",\n",
    "    \"./variable_data/wgi_data/wgi_variables/reg_quality.csv\",\n",
    "    \"./variable_data/wgi_data/wgi_variables/rule_of_law.csv\",\n",
    "    \"./variable_data/wgi_data/wgi_variables/control_of_corruption.csv\",\n",
    "]\n",
    "# Function to extract variables names from filenames   \n",
    "def extract_var_name(filepath):\n",
    "    return filepath.split(\"/\")[-1].replace(\".csv\", \"\")\n",
    "\n",
    "# Function to merge WGI data with the master dataset for a given year\n",
    "def merge_wgi_data(master_df, wgi_file, var_name, year):\n",
    "    wgi_df = pd.read_csv(wgi_file)\n",
    "    wgi_df = wgi_df[[\"country\", f\"{year}_estimate\"]].rename(\n",
    "        columns={f\"{year}_estimate\": f\"{var_name}_{year}\"}\n",
    "    )\n",
    "    # Merge only if the variable does not already exist in the master dataset\n",
    "    if f\"{var_name}_{year}\" not in master_df.columns:\n",
    "        return master_df.merge(wgi_df, on=\"country\", how=\"left\")\n",
    "    return master_df\n",
    "\n",
    "for year, master_df in master_datasets.items():\n",
    "    for wgi_file in wgi_files:\n",
    "        var_name = extract_var_name(wgi_file)\n",
    "        master_datasets[year] = merge_wgi_data(master_datasets[year], wgi_file, var_name, year - 1)\n",
    "\n",
    "# Save the ujpdated master datasets\n",
    "for year, master_df in master_datasets.items():\n",
    "    master_df.to_csv(f\"./processed_data/master_dataset_{year}.csv\", index=False)\n",
    "\n",
    "print(\"Datasets have been updated and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc. housekeeping code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './variable_data/bordering_countries_2017_2018.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m input_filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./variable_data/bordering_countries_2017_2018.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m output_filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./variable_data/bordering_countries_2017_2018_fixed.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m bordering_countries_old \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_filepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Function to convert names to snake_case\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_snake_case\u001b[39m(country_name):\n",
      "File \u001b[0;32m~/coding_projects/nov_2024_replication_project/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coding_projects/nov_2024_replication_project/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/coding_projects/nov_2024_replication_project/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coding_projects/nov_2024_replication_project/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/coding_projects/nov_2024_replication_project/.venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './variable_data/bordering_countries_2017_2018.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "input_filepath = \"./variable_data/bordering_countries_2017_2018.csv\"\n",
    "output_filepath = \"./variable_data/bordering_countries_2017_2018_fixed.csv\"\n",
    "\n",
    "bordering_countries_old = pd.read_csv(input_filepath)\n",
    "\n",
    "\n",
    "# Function to convert names to snake_case\n",
    "def to_snake_case(country_name):\n",
    "    if pd.isna(country_name):  # Handle missing values\n",
    "        return country_name\n",
    "    return (\n",
    "        country_name.replace(\"'\", \"_\")\n",
    "        .replace(\" \", \"_\")\n",
    "        .replace(\"-\", \"_\")\n",
    "        .replace(\",\", \"_\")\n",
    "        .lower()\n",
    "    )\n",
    "\n",
    "\n",
    "# Check if the \"bordering_countries\" column exists\n",
    "if \"bordering_countries\" in bordering_countries_old.columns:\n",
    "    bordering_countries_old[\"bordering_countries\"] = bordering_countries_old[\n",
    "        \"bordering_countries\"\n",
    "    ].apply(\n",
    "        lambda x: \", \".join(\n",
    "            [to_snake_case(country) for country in str(x).split(\", \") if country]\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\"The 'bordering_countries' column is not found in the CSV file.\")\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "bordering_countries_old.to_csv(output_filepath)\n",
    "print(f\"File with snake_case countries saved to {output_filepath}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"nat_resource_list_m_and_a.csv\" using tsicp variable from m_and_a_main_dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
