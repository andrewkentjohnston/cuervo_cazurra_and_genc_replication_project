{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import LDCs greenfield FDI info for years 2017-2018. List of LDCs taken from UNCTAD (2017, 2018). Greenfield obtained from FT fDi Markets database and captures all transactions over USD 500,000. \"Source country\" variable is the country of the ultimate parent, confirmed via email with Juanjo Lopez on 29 October, 2024. 2017_2018_greenfield_data_fdi_markets_original.csv saved as \"main_dataset_greenfield.csv\" and all variables and all country names converted to snake_case to faciliate data wrangling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to ./processed_data/greenfield_main_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from fDi Markets\n",
    "fdi_data = \"./raw_data/2017_2018_greenfield_data_fdi_markets_original.csv\"\n",
    "df = pd.read_csv(fdi_data)\n",
    "\n",
    "\n",
    "# Define function to convert text to snake_case\n",
    "def to_snake_case(s):\n",
    "    return s.strip().replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"'\", \"_\").lower()\n",
    "\n",
    "\n",
    "# Convert variable names (i.e., column headers) to snake_case for data wrangling\n",
    "df.columns = [to_snake_case(col) for col in df.columns]\n",
    "\n",
    "# Convert values in \"destination_country\" and \"source_country\" to snake_case\n",
    "df[\"destination_country\"] = df[\"destination_country\"].apply(to_snake_case)\n",
    "df[\"source_country\"] = df[\"source_country\"].apply(to_snake_case)\n",
    "\n",
    "# Save modified DataFrame to new CSV file\n",
    "output_filepath = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "df.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"File saved to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for instances where source_country == destination_country (i.e., instances of non-FDI). Number should be zero based on description of data from Financial Times fDi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances where source_country == destination_country: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the processed greenfield dataset\n",
    "filepath = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# Check for existence of columns in question\n",
    "if \"source_country\" in df.columns and \"destination_country\" in df.columns:\n",
    "\n",
    "    # Filter rows where source_country == destination_country\n",
    "    non_fdi_rows = df[df[\"source_country\"] == df[\"destination_country\"]]\n",
    "\n",
    "    # Get the count\n",
    "    count = len(non_fdi_rows)\n",
    "\n",
    "    print(f\"Number of instances where source_country == destination_country: {count}\")\n",
    "\n",
    "else:\n",
    "    print(\"The dataset does not contain 'source_country' or 'destination_country'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create spreadsheet of unique destination countries for data auditing/validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to ./audit_data/unique_destinationa_countries.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afghanistan</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angola</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bangladesh</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>benin</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bhutan</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>burkina_faso</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>burundi</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cambodia</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>central_african_republic</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chad</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>democratic_republic_of_congo</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>djibouti</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>east_timor</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ethiopia</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gambia</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>guinea</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>haiti</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>laos</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lesotho</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>liberia</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>madagascar</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>malawi</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mali</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>mauritania</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mozambique</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>myanmar</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>nepal</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>niger</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rwanda</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>senegal</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>sierra_leone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>somalia</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>sudan</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>tanzania</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>togo</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>uganda</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>zambia</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         country  count\n",
       "0                    afghanistan      3\n",
       "1                         angola     11\n",
       "2                     bangladesh     47\n",
       "3                          benin      8\n",
       "4                         bhutan      6\n",
       "5                   burkina_faso      8\n",
       "6                        burundi      4\n",
       "7                       cambodia     92\n",
       "8       central_african_republic      2\n",
       "9                           chad      2\n",
       "10  democratic_republic_of_congo     15\n",
       "11                      djibouti      3\n",
       "12                    east_timor      2\n",
       "13                      ethiopia     54\n",
       "14                        gambia      4\n",
       "15                        guinea     13\n",
       "16                         haiti      3\n",
       "17                          laos     25\n",
       "18                       lesotho      5\n",
       "19                       liberia      8\n",
       "20                    madagascar      5\n",
       "21                        malawi      7\n",
       "22                          mali      5\n",
       "23                    mauritania      2\n",
       "24                    mozambique     45\n",
       "25                       myanmar    117\n",
       "26                         nepal     15\n",
       "27                         niger      2\n",
       "28                        rwanda     17\n",
       "29                       senegal     20\n",
       "30                  sierra_leone      1\n",
       "31                       somalia      3\n",
       "32                         sudan      3\n",
       "33                      tanzania     42\n",
       "34                          togo      3\n",
       "35                        uganda     34\n",
       "36                        zambia     31"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load processed greenfield dataset\n",
    "filepath = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# Get unique country names and count of each\n",
    "unique_destination_countries = df[\"destination_country\"].value_counts().reset_index()\n",
    "\n",
    "# Rename columns\n",
    "unique_destination_countries.columns = [\"country\", \"count\"]\n",
    "\n",
    "# Sort new df alphabetically\n",
    "unique_destination_countries = unique_destination_countries.sort_values(\n",
    "    by=\"country\"\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_filepath = \"./audit_data/unique_destinationa_countries.csv\"\n",
    "unique_destination_countries.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"File saved to {output_filepath}\")\n",
    "\n",
    "# Display results\n",
    "unique_destination_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare list of destination countries with UNCTAD 2017-2018 LDCs list for data audit/validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDCs not appearing as desintation countries in greenfield_main_dataset.csv:\n",
      "comoros\n",
      "eritrea\n",
      "guinea_bissau\n",
      "kiribati\n",
      "sao_tome_and_principe\n",
      "solomon_islands\n",
      "south_sudan\n",
      "tuvalu\n",
      "vanuatu\n",
      "yemen\n",
      "Countries appearing as destination countries in greenfield_main_dataset.csv but not appearing in LDCs list:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both spreadsheets into DataFrames\n",
    "destination_countries_df = pd.read_csv(\"./audit_data/unique_destinationa_countries.csv\")\n",
    "ldcs_df = pd.read_csv(\"./variable_data/ldcs_list_2017_2018.csv\")\n",
    "\n",
    "\n",
    "# Convert country names to snake_case\n",
    "def to_snake_case(s):\n",
    "    return s.strip().replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"'\", \"_\").lower()\n",
    "\n",
    "\n",
    "destination_countries_df[\"country\"] = destination_countries_df[\"country\"].apply(\n",
    "    to_snake_case\n",
    ")\n",
    "ldcs_df[\"country\"] = ldcs_df[\"country\"].apply(to_snake_case)\n",
    "\n",
    "# Convert pandas Series objects into sets for comparison\n",
    "destination_countries_set = set(destination_countries_df[\"country\"])\n",
    "ldcs_set = set(ldcs_df[\"country\"])\n",
    "\n",
    "# Create new sets of missing countries\n",
    "missing_in_destination_countries_list = sorted(ldcs_set - destination_countries_set)\n",
    "missing_in_ldcs_list = sorted(destination_countries_set - ldcs_set)\n",
    "\n",
    "# Display results\n",
    "print(\"LDCs not appearing as desintation countries in greenfield_main_dataset.csv:\")\n",
    "for country in missing_in_destination_countries_list:\n",
    "    print(country)\n",
    "print(\n",
    "    \"Countries appearing as destination countries in greenfield_main_dataset.csv but not appearing in LDCs list:\"\n",
    ")\n",
    "for country in missing_in_ldcs_list:\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDCs not appearing in greenfield FDI are consistent with those for which there was no data in fDi Markets for those years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create spreadsheet of unique source countries for data auditing/validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to ./audit_data/unique_source_countries.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>australia</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>austria</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>azerbaijan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>barbados</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>belgium</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>uae</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ukraine</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>united_kingdom</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>united_states</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>vietnam</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           country  count\n",
       "0        australia      4\n",
       "1          austria      2\n",
       "2       azerbaijan      1\n",
       "3         barbados      1\n",
       "4          belgium      6\n",
       "..             ...    ...\n",
       "58             uae     26\n",
       "59         ukraine      4\n",
       "60  united_kingdom     28\n",
       "61   united_states     49\n",
       "62         vietnam     23\n",
       "\n",
       "[63 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load processed greenfield dataset\n",
    "filepath = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# Get unique source country names and count of each\n",
    "unique_source_countries = df[\"source_country\"].value_counts().reset_index()\n",
    "\n",
    "# Rename columns\n",
    "unique_source_countries.columns = [\"country\", \"count\"]\n",
    "\n",
    "# Sort new df alphabetically\n",
    "unique_source_countries = unique_source_countries.sort_values(by=\"country\").reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "output_filepath = \"./audit_data/unique_source_countries.csv\"\n",
    "unique_source_countries.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"File saved to {output_filepath}\")\n",
    "\n",
    "# Display results\n",
    "unique_source_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare list of source countries with UNCTAD 2017-2018 developed country list for data audit/validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developed countries not appearing as source countries in greenfield_main_dataset.csv:\n",
      "andorra\n",
      "bermuda\n",
      "bulgaria\n",
      "czechia\n",
      "estonia\n",
      "faroe_islands\n",
      "gibraltar\n",
      "greece\n",
      "greenland\n",
      "holy_see\n",
      "hungary\n",
      "iceland\n",
      "ireland\n",
      "latvia\n",
      "lithuania\n",
      "luxembourg\n",
      "malta\n",
      "poland\n",
      "romania\n",
      "saint_pierre_and_miquelon\n",
      "san_marino\n",
      "slovakia\n",
      "slovenia\n",
      "sweden\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both spreadsheets into DataFrames\n",
    "source_countries_df = pd.read_csv(\"./audit_data/unique_source_countries.csv\")\n",
    "developed_countries_df = pd.read_csv(\n",
    "    \"./variable_data/developed_countries_2017_2018.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "# Convert country names to snake_case\n",
    "def to_snake_case(s):\n",
    "    return s.strip().replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"'\", \"_\").lower()\n",
    "\n",
    "\n",
    "source_countries_df[\"country\"] = source_countries_df[\"country\"].apply(to_snake_case)\n",
    "developed_countries_df[\"country\"] = developed_countries_df[\"country\"].apply(\n",
    "    to_snake_case\n",
    ")\n",
    "\n",
    "# Convert pandas Series objects into sets for comparison\n",
    "source_countries_set = set(source_countries_df[\"country\"])\n",
    "developed_countries_set = set(developed_countries_df[\"country\"])\n",
    "\n",
    "# Create new set of missing countries\n",
    "missing_in_source_countries = developed_countries_set - source_countries_set\n",
    "\n",
    "# Display results\n",
    "print(\n",
    "    \"Developed countries not appearing as source countries in greenfield_main_dataset.csv:\"\n",
    ")\n",
    "for country in sorted(missing_in_source_countries):\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developed countries not appearing in greenfield_main_dataset were checked manually against unique_source_countries.csv to ensure no matches were missed due to spelling or punctuation issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare list of countries in tax_havens.csv with countries appearing in source_country column and identify any tax_haven countries not appearing as source countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tax haven countries not appearing as source countries in greenfield_main_dataset.csv:\n",
      "andorra\n",
      "anguilla\n",
      "antigua_and_barbuda\n",
      "aruba\n",
      "bahamas\n",
      "bahrain\n",
      "bermuda\n",
      "british_virgin_islands\n",
      "cayman_islands\n",
      "gibraltar\n",
      "grenada\n",
      "guernsey\n",
      "ireland\n",
      "isle_of_man\n",
      "jersey\n",
      "lebanon\n",
      "liechtenstein\n",
      "luxembourg\n",
      "macau\n",
      "malta\n",
      "marshall_islands\n",
      "monaco\n",
      "netherlands_antilles\n",
      "panama\n",
      "puerto_rico\n",
      "samoa\n",
      "seychelles\n",
      "st_kitts_and_nevis\n",
      "st_lucia\n",
      "st_vincent_and_grenadines\n",
      "turks_and_caicos\n",
      "vanuatu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both datasets\n",
    "source_countries_df = pd.read_csv(\"./audit_data/unique_source_countries.csv\")\n",
    "tax_havens_df = pd.read_csv(\"./variable_data/tax_havens.csv\")\n",
    "\n",
    "\n",
    "# Covert country names to snake_case\n",
    "def to_snake_case(s):\n",
    "    return s.strip().replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"'\", \"_\").lower()\n",
    "\n",
    "\n",
    "source_countries_df[\"countries\"] = source_countries_df[\"country\"].apply(to_snake_case)\n",
    "tax_havens_df[\"country\"] = tax_havens_df[\"country\"].apply(to_snake_case)\n",
    "\n",
    "# Convert Pandas Series objects into sets for comparison\n",
    "source_countries_set = set(source_countries_df[\"country\"])\n",
    "tax_havens_set = set(tax_havens_df[\"country\"])\n",
    "\n",
    "# Create set of missing countries\n",
    "missing_tax_havens_set = tax_havens_set - source_countries_set\n",
    "\n",
    "# Display results\n",
    "print(\n",
    "    \"Tax haven countries not appearing as source countries in greenfield_main_dataset.csv:\"\n",
    ")\n",
    "\n",
    "for country in sorted(missing_tax_havens_set):\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tax haven countries not appearing in greenfield_main_dataset were checked manually against unique_source_countries.csv to ensure no matches were missed due to spelling or punctuation issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create source_is_th variable. Value will be True if source_country appears in list of tax_havens and False otherwise. Tax havens list taken from footnote 10 on page 1506 of Tørsløv, T., Wier, L., & Zucman, G. (2023). The missing profits of nations. The Review of Economic Studies, 90(3), 1499-1534."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greenfield_main_dataset.csv updated with \"source_is_th\" variable.\n",
      "Number of observations where \"source_is_th\" is True: 96\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the spreadsheets into DataFrames\n",
    "\n",
    "greenfield_df = pd.read_csv(\"./processed_data/greenfield_main_dataset.csv\")\n",
    "tax_havens_df = pd.read_csv(\"./variable_data/tax_havens.csv\")\n",
    "\n",
    "# Create set of tax havens\n",
    "tax_havens_set = set(tax_havens_df[\"country\"])\n",
    "\n",
    "# Add \"source_is_th\" variable to greenfield_df\n",
    "greenfield_df[\"source_is_th\"] = greenfield_df[\"source_country\"].isin(tax_havens_set)\n",
    "\n",
    "greenfield_df.to_csv(\"./processed_data/greenfield_main_dataset.csv\", index=False)\n",
    "\n",
    "print('greenfield_main_dataset.csv updated with \"source_is_th\" variable.')\n",
    "\n",
    "# Output the number of observations where \"source_is_th\" is True to console\n",
    "true_count = greenfield_df[\"source_is_th\"].sum()\n",
    "print(f'Number of observations where \"source_is_th\" is True: {true_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create destination_is_th variable. Value will be True if destination_country appears in list of tax_havens and False otherwise. Tax havens list taken from footnote 10 on page 1506 of Tørsløv, T., Wier, L., & Zucman, G. (2023). The missing profits of nations. The Review of Economic Studies, 90(3), 1499-1534."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greenfield_main_dataset.csv updated with \"destination_is_th\" variable.\n",
      "Number of observations where \"destination_is_th\" is True: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the spreadsheets into DataFrames\n",
    "\n",
    "greenfield_df = pd.read_csv(\"./processed_data/greenfield_main_dataset.csv\")\n",
    "tax_havens_df = pd.read_csv(\"./variable_data/tax_havens.csv\")\n",
    "\n",
    "# Create set of tax havens\n",
    "tax_havens_set = set(tax_havens_df[\"country\"])\n",
    "\n",
    "# Add \"destination_is_th\" variable to greenfield_df\n",
    "greenfield_df[\"destination_is_th\"] = greenfield_df[\"destination_country\"].isin(\n",
    "    tax_havens_set\n",
    ")\n",
    "\n",
    "greenfield_df.to_csv(\"./processed_data/greenfield_main_dataset.csv\", index=False)\n",
    "\n",
    "print('greenfield_main_dataset.csv updated with \"destination_is_th\" variable.')\n",
    "\n",
    "# Output the number of observations where \"destination_is_th\" is True to console\n",
    "true_count = greenfield_df[\"destination_is_th\"].sum()\n",
    "print(f'Number of observations where \"destination_is_th\" is True: {true_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset comprising all observations where source_country is a tax haven and manually check accuracy as true origin of FDI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to greenfield_source_is_th_unique.csv\n",
      "Number of unique parent companies in filtered data: 63\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "greenfield_df = pd.read_csv(\"./processed_data/greenfield_main_dataset.csv\")\n",
    "\n",
    "# Filter for those instances were source_is_th == True\n",
    "greenfield_source_is_th_df = greenfield_df[greenfield_df[\"source_is_th\"] == True]\n",
    "\n",
    "# Select specific columns\n",
    "selected_columns = [\n",
    "    \"parent_company\",\n",
    "    \"company_profile\",\n",
    "    \"website\",\n",
    "    \"source_country\",\n",
    "    \"sector\",\n",
    "    \"sub_sector\",\n",
    "    \"cluster\",\n",
    "    \"activity\",\n",
    "]\n",
    "\n",
    "# Retain only unique rows based on 'parent_company' and selected columns\n",
    "unique_parent_company_df = greenfield_source_is_th_df[selected_columns].drop_duplicates(\n",
    "    subset=[\"parent_company\"]\n",
    ")\n",
    "\n",
    "# Convert new dataframe to CSV for manual checking\n",
    "unique_parent_company_df.to_csv(\n",
    "    \"./audit_data/greenfield_source_is_th_unique.csv\", index=False\n",
    ")\n",
    "\n",
    "num_observations = unique_parent_company_df.shape[0]\n",
    "\n",
    "print(\"Filtered data saved to greenfield_source_is_th_unique.csv\")\n",
    "print(f\"Number of unique parent companies in filtered data: {num_observations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually check all companies to determine accuracy of parent country. Results/comments saved to \"./audit_data/source_country_th_man_flagged.csv\". Companies with source countries identified as tax havens were checked manually on S&P Capital IQ, Orbis, and on company websites. In instances where the stated source country seemed to be either incorrect, or seemed questionable, source_country_questionable is flagged as TRUE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new dataset containing only parent companies where source country is questionable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitlered data saved to ./audit_data/greenfield_source_questionable.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "filepath = \"./audit_data/source_country_th_man_flagged.csv\"\n",
    "data = pd.read_csv(filepath)\n",
    "\n",
    "# Filter rows where 'source_country_questionable' is True\n",
    "filtered_data = data[data[\"source_country_questionable\"] == True]\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "filtered_filepath = \"./audit_data/greenfield_source_questionable.csv\"\n",
    "filtered_data.to_csv(filtered_filepath, index=False)\n",
    "\n",
    "print(f\"Fitlered data saved to {filtered_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use flagged data to create a variable source_questionable in the greenfield_main_dataset.csv. Value is True if the source country has been flagged as questionable in the greenfield_source_questionable_csv and false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset saved to ./processed_data/greenfield_main_dataset.csv\n",
      "Number of observations with source_questionable = True: 22\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "main_dataset_filepath = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "questionable_source_filepath = \"./audit_data/greenfield_source_questionable.csv\"\n",
    "\n",
    "main_dataset = pd.read_csv(main_dataset_filepath)\n",
    "questionable_source = pd.read_csv(questionable_source_filepath)\n",
    "\n",
    "# Extract list of parent companies from questionable source dataset\n",
    "questionable_parent_companies = questionable_source[\"parent_company\"].unique()\n",
    "\n",
    "# Create the \"source_questionable\" column in main dataset\n",
    "main_dataset[\"source_questionable\"] = main_dataset[\"parent_company\"].isin(\n",
    "    questionable_parent_companies\n",
    ")\n",
    "\n",
    "# Save the updated dataset to CSV file\n",
    "main_dataset.to_csv(main_dataset_filepath, index=False)\n",
    "\n",
    "print(f\"Updated dataset saved to {main_dataset_filepath}\")\n",
    "\n",
    "true_count = main_dataset[\"source_questionable\"].sum()\n",
    "print(f\"Number of observations with source_questionable = True: {true_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 observations flagged in the greenfield dataset where the true source country is questionable after manual checking of all instances where \"source_country\" is a tax haven (see above re: list of tax haven countries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"developed\" variable in greenfield_main_dataset.csv. Variable takes value of True if source country appears in list of developed countries and False otherwise. Also created \"emne\" variable that takes the inverse value of the \"developed\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greenfield_main_dataset.csv updated with \"developed\" and \"emne\" variables.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load spreadsheets into DataFrames\n",
    "\n",
    "greenfield_filepath = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "developed_countries_filepath = \"./variable_data/developed_countries_2017_2018.csv\"\n",
    "\n",
    "greenfield_df = pd.read_csv(greenfield_filepath)\n",
    "developed_countries_df = pd.read_csv(developed_countries_filepath)\n",
    "\n",
    "# Create set of developed countries\n",
    "developed_countries_set = set(developed_countries_df[\"country\"])\n",
    "\n",
    "# Add \"developed\" variable to greenfield_df\n",
    "greenfield_df[\"developed\"] = greenfield_df[\"source_country\"].isin(\n",
    "    developed_countries_set\n",
    ")\n",
    "\n",
    "# Add \"emne\" variable to greenfield_df\n",
    "greenfield_df[\"emne\"] = ~greenfield_df[\"source_country\"].isin(developed_countries_set)\n",
    "\n",
    "# Save the updated DataFrame as greenfield_main_dataset.csv\n",
    "greenfield_df.to_csv(greenfield_filepath, index=False)\n",
    "\n",
    "print(f'greenfield_main_dataset.csv updated with \"developed\" and \"emne\" variables.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"bordering_country\" variable. Variable is True if source countries shares a border with destination country and False otherwise. Bordering countries checked from CIA World Factbook https://www.cia.gov/the-world-factbook/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new variable \"bordering country\" has been added and the updated file has been saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load spreadsheets into DataFrames\n",
    "greenfield_filepath = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "bordering_countries_filepath = \"./variable_data/bordering_countries_2017_2018.csv\"\n",
    "\n",
    "greenfield_df = pd.read_csv(greenfield_filepath)\n",
    "bordering_countries_df = pd.read_csv(bordering_countries_filepath)\n",
    "\n",
    "# Create dictionary of bordering countries\n",
    "bordering_countries_dict = bordering_countries_df.set_index(\"ldc\")[\n",
    "    \"bordering_countries\"\n",
    "].to_dict()\n",
    "\n",
    "\n",
    "# Function to check if the source country is in the list of bordering countries\n",
    "def is_bordering(source_country, destination_country):\n",
    "    bordering_countries = bordering_countries_dict.get(destination_country, \"\")\n",
    "    if pd.isna(bordering_countries) or not bordering_countries:\n",
    "        return False\n",
    "    bordering_list = bordering_countries.split(\", \")\n",
    "    return source_country in bordering_list\n",
    "\n",
    "\n",
    "# Apply the is_bordering function to create the new \"bordering_country\" variable in greenfield_df\n",
    "greenfield_df[\"bordering_country\"] = greenfield_df.apply(\n",
    "    lambda row: is_bordering(row[\"source_country\"], row[\"destination_country\"]), axis=1\n",
    ")\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "greenfield_df.to_csv(greenfield_filepath, index=False)\n",
    "\n",
    "print(\n",
    "    f'The new variable \"bordering country\" has been added and the updated file has been saved.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new \"industry\" variable by joining \"sector\", \"sub_sector\", and \"activity\". Create list/spreadsheet of all unique values in \"industry\" column (and their count) from greenfield_main_dataset.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset has been saved to ./processed_data/greenfield_main_dataset.csv.\n",
      "        Industry counts has been saved to ./audit_data/industry_counts.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load processed greenfield dataset\n",
    "filepath = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "\n",
    "# Define function to convert strings to snake_case\n",
    "def to_snake_case(s):\n",
    "    s = (\n",
    "        s.replace(\" \", \"_\")\n",
    "        .replace(\"'\", \"_\")\n",
    "        .replace(\"&\", \"and\")\n",
    "        .replace(\",\", \"\")\n",
    "        .replace(\"/\", \"\")\n",
    "        .replace(\"-\", \"_\")\n",
    "        .replace(\",\", \"\")\n",
    "        .replace(\"(\", \"\")\n",
    "        .replace(\")\", \"\")\n",
    "    )\n",
    "    return s.lower()\n",
    "\n",
    "\n",
    "# Create a new 'industry' column by joining 'sector', 'sub-sector', and 'activity'\n",
    "df[\"industry\"] = (\n",
    "    df[[\"sector\", \"sub_sector\", \"activity\"]]\n",
    "    .fillna(\"\")\n",
    "    .apply(lambda x: \" \".join(x), axis=1)\n",
    ")\n",
    "\n",
    "# Convert the new 'industry' column to snake_case\n",
    "df[\"industry\"] = df[\"industry\"].apply(to_snake_case)\n",
    "\n",
    "# Save the modified DataFrame to CSV\n",
    "df.to_csv(filepath, index=False)\n",
    "\n",
    "# Calculate the count of unique values in 'industry' column\n",
    "industry_counts = df[\"industry\"].value_counts().reset_index()\n",
    "industry_counts.columns = [\"industry\", \"count\"]\n",
    "\n",
    "# Sort alphabetically\n",
    "industry_counts.sort_values(by=\"industry\", inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "industry_counts_filepath = \"./audit_data/industry_counts.csv\"\n",
    "industry_counts.to_csv(industry_counts_filepath, index=False)\n",
    "\n",
    "print(\n",
    "    f\"Updated dataset has been saved to {filepath}.\\n\\\n",
    "        Industry counts has been saved to {industry_counts_filepath}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"natural_resource\" variable. Variable is True if industry is listed as a natural resource industry in the \"nat_resource_list.csv\" spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The modified dataset has been saved to ./processed_data/greenfield_main_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both spreadsheets into DataFrames\n",
    "main_dataset_path = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "nat_resource_list_path = \"./variable_data/nat_resource_list.csv\"\n",
    "\n",
    "main_df = pd.read_csv(main_dataset_path)\n",
    "nat_resource_df = pd.read_csv(nat_resource_list_path)\n",
    "\n",
    "# Create a set of natural resource industries\n",
    "natural_resource_set = set(\n",
    "    nat_resource_df[nat_resource_df[\"natural_resource\"] == True][\"industry\"]\n",
    ")\n",
    "\n",
    "# Create a new column \"natural_resource_ind\" in main_df\n",
    "main_df[\"natural_resource_ind\"] = main_df[\"industry\"].apply(\n",
    "    lambda x: x in natural_resource_set\n",
    ")\n",
    "\n",
    "# Save the modified DataFrame to CSV\n",
    "main_df.to_csv(main_dataset_path, index=False)\n",
    "\n",
    "print(f\"The modified dataset has been saved to {main_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"colonial_link\" variable. Variable is True if the source country is listed in the \"colonial_rulers\" column for that country in \"colonial_rulers.csv\" and False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset saved to ./processed_data/greenfield_main_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both spreadsheets into DataFrames\n",
    "main_dataset_path = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "colonial_rulers_path = \"./variable_data/colonial_rulers_list.csv\"\n",
    "\n",
    "main_df = pd.read_csv(main_dataset_path)\n",
    "colonial_rulers_df = pd.read_csv(colonial_rulers_path)\n",
    "\n",
    "# Create a dictionary from colonial_rulers_list\n",
    "colonial_rulers_dict = colonial_rulers_df.set_index(\"country\")[\n",
    "    \"colonial_rulers\"\n",
    "].to_dict()\n",
    "\n",
    "\n",
    "# Function to check for colonial link\n",
    "def has_colonial_link(row):\n",
    "    destination = row[\"destination_country\"]\n",
    "    source = row[\"source_country\"]\n",
    "    colonial_rulers = colonial_rulers_dict.get(destination)\n",
    "\n",
    "    if isinstance(colonial_rulers, str):\n",
    "        colonial_rulers_list = [\n",
    "            ruler.strip().replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"'\", \"_\").lower()\n",
    "            for ruler in colonial_rulers.split(\", \")\n",
    "        ]\n",
    "        return source in colonial_rulers_list\n",
    "    return False\n",
    "\n",
    "\n",
    "# Apply the function to create new \"colonial_link\" column\n",
    "main_df[\"colonial_link\"] = main_df.apply(has_colonial_link, axis=1)\n",
    "\n",
    "# Save the updated DatafRame to CSV\n",
    "main_df.to_csv(main_dataset_path, index=False)\n",
    "\n",
    "print(f\"Updated dataset saved to {main_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 2017 and 2018 spreadsheets for each individual LDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country spreadsheets created and saved to \"country_spreadsheets/greenfield\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load both spreadsheets into DataFrames\n",
    "main_dataset_path = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "ldcs_list_path = \"./variable_data/ldcs_list_2017_2018.csv\"\n",
    "\n",
    "main_df = pd.read_csv(main_dataset_path)\n",
    "ldcs_df = pd.read_csv(ldcs_list_path)\n",
    "\n",
    "\n",
    "# Define function to convert country names to snake_case\n",
    "def to_snake_case(s):\n",
    "    return s.strip().replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"'\", \"_\").lower()\n",
    "\n",
    "\n",
    "# Convert country names in both DataFrames to snake_case\n",
    "main_df[\"destination_country\"] = main_df[\"destination_country\"].apply(to_snake_case)\n",
    "ldcs_df[\"country\"] = ldcs_df[\"country\"].apply(to_snake_case)\n",
    "\n",
    "# Convert \"project_date\" to datetime for easier filtering\n",
    "main_df[\"project_date\"] = pd.to_datetime(main_df[\"project_date\"], format=\"%b %Y\")\n",
    "\n",
    "# Extract year from \"project_date\"\n",
    "main_df[\"year\"] = main_df[\"project_date\"].dt.year\n",
    "\n",
    "# Create output directories for country spreadsheets\n",
    "output_dir = \"./country_spreadsheets/greenfield\"\n",
    "\n",
    "for year in [2017, 2018]:\n",
    "    year_dir = os.path.join(output_dir, str(year))\n",
    "    if not os.path.exists(year_dir):\n",
    "        os.makedirs(year_dir)\n",
    "\n",
    "# Create list to keep track of rows not assigned to a spreadhseet\n",
    "unassigned_rows = []\n",
    "\n",
    "# List to keep track of empty spreadsheets\n",
    "empty_spreadsheets = []\n",
    "\n",
    "# Proess each country in the ldcs_df\n",
    "for country in ldcs_df[\"country\"]:\n",
    "    for year in [2017, 2018]:\n",
    "        # Filter data for the specific country and year\n",
    "        country_year_data = main_df[\n",
    "            (main_df[\"destination_country\"] == country) & (main_df[\"year\"] == year)\n",
    "        ]\n",
    "\n",
    "        # Create filename\n",
    "        year_dir = os.path.join(output_dir, str(year))\n",
    "        filename = f\"{year_dir}/{country}_greenfield_{year}.csv\"\n",
    "\n",
    "        # Save data to CSV\n",
    "        if not country_year_data.empty:\n",
    "            country_year_data.to_csv(filename, index=False)\n",
    "        else:\n",
    "            # Save an empty CSV with the same headers as greenfield_main_dataset.csv\n",
    "            empty_df = main_df.head(0)\n",
    "            empty_df.to_csv(filename, index=False)\n",
    "            empty_spreadsheets.append(filename)\n",
    "\n",
    "# Identify rows not assigned to any country spreadsheet\n",
    "assigned_rows = main_df[main_df[\"destination_country\"].isin(ldcs_df[\"country\"])]\n",
    "unassigned_rows = main_df[~main_df.index.isin(assigned_rows.index)]\n",
    "\n",
    "# Save unassigned rows to a CSV file\n",
    "audit_dir = \"./audit_data\"\n",
    "unassigned_filename = f\"{audit_dir}/unassigned_rows.csv\"\n",
    "unassigned_rows.to_csv(unassigned_filename, index=False)\n",
    "\n",
    "# Save list of empty spreadsheets to a CSV file\n",
    "empty_spreadsheets_filename = f\"{audit_dir}/empty_spreadsheets.csv\"\n",
    "pd.DataFrame(empty_spreadsheets, columns=[\"empty_spreadsheets\"]).to_csv(\n",
    "    empty_spreadsheets_filename, index=False\n",
    ")\n",
    "\n",
    "print('Country spreadsheets created and saved to \"country_spreadsheets/greenfield\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding M&A data from SDC Platinum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load m_and_a_dataset. Data was acquired from SDC's Mergers and Acquisitions database via WRDS. Because the acquirer ultimate parent nation (AUPNATION) variable isn't included in the WRDS version of SDC M&A, the AUPNATION variables were purchased separately, directly from LSEG/Refinitiv and added to the dataset once all non-LDC destinations were filtered out (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a spreadsheet of all unique values appearing in \"TNATION\" (target nation) column in order to check against list of LDCs for different country spellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values saved to ./audit_data/m_and_a_unique_destination_countries.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "input_filepath = \"./raw_data/m_and_a_data_2017_2018.csv\"\n",
    "output_filepath = \"./audit_data/m_and_a_unique_destination_countries.csv\"\n",
    "\n",
    "data = pd.read_csv(input_filepath)\n",
    "\n",
    "# Get unique values and their counts from TNATION column\n",
    "unique_values_counts = data[\"TNATION\"].value_counts().reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "unique_values_counts.columns = [\"country\", \"count\"]\n",
    "\n",
    "# Sort df alphabetically\n",
    "unique_values_counts = unique_values_counts.sort_values(by=\"country\")\n",
    "# Save the new dataframe to a CSV file\n",
    "unique_values_counts.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"Unique values saved to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare list of unique destination countries with UNCTAD 2017-2018 LDCs list for data audit/validation purposes. A \"name conversion\" spreadsheet was created in order to convert all names of LDC countries to the same spelling/format as those used in earlier scripts above. In the next step, non-LDC destination countries (\"TNATION\") are filtered out of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out non-LDC destination countries from M&A data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing LDCs: 6\n",
      "LDC countries not appearing in the M&A dataset's destination_country column:\n",
      "comoros\n",
      "guinea_bissau\n",
      "kiribati\n",
      "sao_tome_and_principe\n",
      "south_sudan\n",
      "tuvalu\n",
      "Filtered data saved to ./processed_data/m_and_a_main_dataset.csv.\n",
      "Missing LDC countries saved to ./audit_data/m_and_a_missing_ldcs.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "m_and_a_data = pd.read_csv(\"./raw_data/m_and_a_data_2017_2018.csv\")\n",
    "name_conversions = pd.read_csv(\"./audit_data/m_and_a_name_conversions.csv\")\n",
    "ldcs_list = pd.read_csv(\"./variable_data/ldcs_list_2017_2018.csv\")\n",
    "\n",
    "\n",
    "# Function to standardize country names and convert to snake_case\n",
    "def standardize_country_names(data_df, conversion_df):\n",
    "    # Apply name mapping\n",
    "    name_map = conversion_df.set_index(\"old_name\")[\"new_name\"].dropna().to_dict()\n",
    "    data_df[\"destination_country\"] = data_df[\"TNATION\"].replace(name_map)\n",
    "\n",
    "    # Then convert the names to snake_case\n",
    "    data_df[\"destination_country\"] = (\n",
    "        data_df[\"destination_country\"]\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\", regex=False)\n",
    "        .str.replace(\"-\", \"_\", regex=False)\n",
    "        .str.replace(\",\", \"_\", regex=False)\n",
    "        .str.replace(\"'\", \"_\", regex=False)\n",
    "    )\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "# Apply the name conversion function to M&A dataframe\n",
    "m_and_a_data = standardize_country_names(m_and_a_data, name_conversions)\n",
    "\n",
    "# Filter out all entries for which the target nation is not an LDC\n",
    "filtered_m_and_a_data = m_and_a_data[\n",
    "    m_and_a_data[\"destination_country\"].isin(ldcs_list[\"country\"])\n",
    "]\n",
    "\n",
    "# Identify LDC countries that do not appear in the M&A dataset\n",
    "ldc_countries_set = set(ldcs_list[\"country\"])\n",
    "destination_countries_set = set(\n",
    "    filtered_m_and_a_data[\"destination_country\"].dropna().unique()\n",
    ")\n",
    "missing_countries = sorted(ldc_countries_set - destination_countries_set)\n",
    "\n",
    "# Print missing countries\n",
    "print(f\"Number of missing LDCs: {len(missing_countries)}\")\n",
    "print(\"LDC countries not appearing in the M&A dataset's destination_country column:\")\n",
    "for country in missing_countries:\n",
    "    print(country)\n",
    "\n",
    "# Save the list of missing countries to a CSV file\n",
    "missing_countries_filepath = \"./audit_data/m_and_a_missing_ldcs.csv\"\n",
    "missing_countries_df = pd.DataFrame(missing_countries, columns=[\"missing_countries\"])\n",
    "missing_countries_df.to_csv(missing_countries_filepath, index=False)\n",
    "\n",
    "# Save the filtered data\n",
    "output_filepath = \"./processed_data/m_and_a_main_dataset.csv\"\n",
    "filtered_m_and_a_data.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to {output_filepath}.\")\n",
    "print(f\"Missing LDC countries saved to {missing_countries_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDCs not appearing in filtered dataset were manually checked against previously created list of unique TNATION values to ensure none were missed due to typos or spelling differences. LDCs listed above were manually confirmed to be absent from the M & A data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of unique countries appearing in \"destination_country\" and counts, as well as total size of filtered dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of LDCs listed in ldcs_list_2017_2018: 47\n",
      "Number of unique values appearing in 'destination_country': 41\n",
      "Unique values in 'destination_country:\n",
      "destination_country\n",
      "afghanistan                      3\n",
      "angola                          15\n",
      "bangladesh                      23\n",
      "benin                            5\n",
      "bhutan                           1\n",
      "burkina_faso                    14\n",
      "burundi                          3\n",
      "cambodia                        50\n",
      "central_african_republic         2\n",
      "chad                             4\n",
      "democratic_republic_of_congo     9\n",
      "djibouti                         4\n",
      "east_timor                       2\n",
      "eritrea                          1\n",
      "ethiopia                        13\n",
      "gambia                           1\n",
      "guinea                           2\n",
      "haiti                            4\n",
      "laos                            13\n",
      "lesotho                          7\n",
      "liberia                          2\n",
      "madagascar                      10\n",
      "malawi                           7\n",
      "mali                            10\n",
      "mauritania                       4\n",
      "mozambique                      13\n",
      "myanmar                         45\n",
      "nepal                           16\n",
      "niger                            3\n",
      "rwanda                           7\n",
      "senegal                         16\n",
      "sierra_leone                     2\n",
      "solomon_islands                  2\n",
      "somalia                          1\n",
      "sudan                            4\n",
      "tanzania                        26\n",
      "togo                             8\n",
      "uganda                          18\n",
      "vanuatu                          3\n",
      "yemen                            2\n",
      "zambia                          31\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of observations in filtered data: 406\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "m_and_a_dataset = pd.read_csv(\"./processed_data/m_and_a_main_dataset.csv\")\n",
    "ldcs_list = pd.read_csv(\"./variable_data/ldcs_list_2017_2018.csv\")\n",
    "\n",
    "# Get unique values appearing in \"destination_country\" and their counts\n",
    "destination_counts = m_and_a_dataset[\"destination_country\"].value_counts().sort_index()\n",
    "\n",
    "print(f\"Number of LDCs listed in ldcs_list_2017_2018: {len(ldcs_list)}\")\n",
    "print(\n",
    "    f\"Number of unique values appearing in 'destination_country': {len(destination_counts)}\"\n",
    ")\n",
    "print(\"Unique values in 'destination_country:\")\n",
    "print(destination_counts)\n",
    "\n",
    "# Get the total number of observations\n",
    "total_observations = len(m_and_a_dataset)\n",
    "print(\"\\nTotal number of observations in filtered data:\", total_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of unique source countries in filtered dataset (41) tallies with number of LDCs (47) minus those identified as missing (6). Dataset has been filtered down to just those transactions with LDCs as destination countries. Next step is to get ultimate parent country for all entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acquirer ultimate parent nation information obtained from Refinitiv/LSEG for filtered dataset and saved as \"./raw_data/sdc_m_and_a_data_incl_aupn.xlsx\". Acquirer ultimate parent nation saved under variable \"AUPNATION\". Previous m_and_a_main_dataset.csv moved to \"./archived_data/m_and_a_main_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error moving filed: Destination path './archived_data/m_and_a_main_dataset.csv' already exists\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "source_file = \"./processed_data/m_and_a_main_dataset.csv\"\n",
    "destination_directory = \"./archived_data\"\n",
    "\n",
    "\n",
    "def move_file(source_path, destination_dir):\n",
    "    try:\n",
    "        # Check if source file exists\n",
    "        if not os.path.isfile(source_path):\n",
    "            print(f\"The file '{source_path} does not exist.\")\n",
    "            return\n",
    "\n",
    "        # Check if the destination directory exists; if not, create it\n",
    "        if not os.path.isdir(destination_dir):\n",
    "            os.makedirs(destination_dir)\n",
    "            print(f\"Created directory '{destination_dir}\")\n",
    "\n",
    "        # Move the file\n",
    "        shutil.move(source_path, destination_dir)\n",
    "        print(f\"File '{source_path}' moved to '{destination_dir}' successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error moving filed: {e}\")\n",
    "\n",
    "\n",
    "move_file(source_file, destination_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with new dataset containing AUPNATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize new dataset variable names and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to ./processed_data/m_and_a_main_dataset.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load M & A dataset\n",
    "m_and_a_data = \"./raw_data/sdc_m_and_a_data_incl_aupn.csv\"\n",
    "df = pd.read_csv(m_and_a_data)\n",
    "\n",
    "\n",
    "# Define function to convert text to snake_case\n",
    "def to_snake_case(s):\n",
    "    return s.strip().replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"'\", \"_\").lower()\n",
    "\n",
    "\n",
    "# Convert variable names (i.e., column headers) to snake_case for data wrangling\n",
    "df.columns = [to_snake_case(col) for col in df.columns]\n",
    "\n",
    "# Create new variables \"destination_country\" and \"source_country\" containing converted values from \"AUPNATION\" and \"TNATION\" respectively.\n",
    "df[\"destination_country\"] = df[\"destination_country\"].apply(to_snake_case)\n",
    "df[\"source_country\"] = df[\"aupnation\"].apply(to_snake_case)\n",
    "\n",
    "# Save modified DataFrame to new CSV file\n",
    "output_filepath = \"./processed_data/m_and_a_main_dataset.csv\"\n",
    "df.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"File saved to {output_filepath}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check for missing LDCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of LDCs: 47\n",
      "Number of unique destination countries in new dataset: 41\n",
      "Number of missing LDCs in new dataset: 6\n",
      "LDC countries not appearing in new M&A dataset destination_country column:\n",
      "comoros\n",
      "guinea_bissau\n",
      "kiribati\n",
      "sao_tome_and_principe\n",
      "south_sudan\n",
      "tuvalu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "m_and_a_unique_set = set(\n",
    "    pd.read_csv(\"./processed_data/m_and_a_main_dataset.csv\")[\n",
    "        \"destination_country\"\n",
    "    ].unique()\n",
    ")\n",
    "ldcs_set = set(pd.read_csv(\"./variable_data/ldcs_list_2017_2018.csv\")[\"country\"])\n",
    "\n",
    "missing_ldcs_set = sorted(ldcs_set - m_and_a_unique_set)\n",
    "\n",
    "print(f\"Number of LDCs: {len(ldcs_set)}\")\n",
    "print(\n",
    "    f\"Number of unique destination countries in new dataset: {len(m_and_a_unique_set)}\"\n",
    ")\n",
    "print(f\"Number of missing LDCs in new dataset: {len(missing_ldcs_set)}\")\n",
    "print(f\"LDC countries not appearing in new M&A dataset destination_country column:\")\n",
    "for country in missing_ldcs_set:\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDCs missing in new data that includes AUPNATION aligns with count above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out all observations except those where deal status is \"complete\"  and replace the previous file as \"m_and_a_main_dataset.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset saved to ./processed_data/m_and_a_main_dataset.csv.\n",
      "Number of observations removed from dataset: 103\n",
      "Number of rows after filtering: 303\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "m_and_a_data = pd.read_csv(\"./processed_data/m_and_a_main_dataset.csv\")\n",
    "\n",
    "# Record the initial number of rows\n",
    "initial_row_count = len(m_and_a_data)\n",
    "\n",
    "# Filter out observations where STATUSCODE is either C or P\n",
    "m_and_a_data = m_and_a_data[\n",
    "    (m_and_a_data[\"statuscode\"] == \"C\")\n",
    "]\n",
    "\n",
    "# Record the number of rows after filtering\n",
    "filtered_row_count = len(m_and_a_data)\n",
    "rows_removed = initial_row_count - filtered_row_count\n",
    "\n",
    "# Save the filtered dataset\n",
    "output_filepath = \"./processed_data/m_and_a_main_dataset.csv\"\n",
    "m_and_a_data.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"Filtered dataset saved to {output_filepath}.\")\n",
    "print(f\"Number of observations removed from dataset: {rows_removed}\")\n",
    "print(f\"Number of rows after filtering: {filtered_row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new list of missing LDCs after filtering deals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of LDCs: 47\n",
      "Number of unique destination countries in new dataset: 37\n",
      "Number of missing LDCs in new dataset: 10\n",
      "LDC countries not appearing in new M&A dataset destination_country column:\n",
      "comoros\n",
      "gambia\n",
      "guinea_bissau\n",
      "kiribati\n",
      "liberia\n",
      "sao_tome_and_principe\n",
      "solomon_islands\n",
      "somalia\n",
      "south_sudan\n",
      "tuvalu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "m_and_a_unique_set = set(\n",
    "    pd.read_csv(\"./processed_data/m_and_a_main_dataset.csv\")[\n",
    "        \"destination_country\"\n",
    "    ].unique()\n",
    ")\n",
    "ldcs_set = set(pd.read_csv(\"./variable_data/ldcs_list_2017_2018.csv\")[\"country\"])\n",
    "\n",
    "missing_ldcs_set = sorted(ldcs_set - m_and_a_unique_set)\n",
    "\n",
    "print(f'Number of LDCs: {len(ldcs_set)}')\n",
    "print(\n",
    "    f\"Number of unique destination countries in new dataset: {len(m_and_a_unique_set)}\"\n",
    ")\n",
    "print(f\"Number of missing LDCs in new dataset: {len(missing_ldcs_set)}\")\n",
    "print(f\"LDC countries not appearing in new M&A dataset destination_country column:\")\n",
    "for country in missing_ldcs_set:\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create spreadsheet of unique source countries (AUPNATION) for data auditing/validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to ./audit_data/m_and_a_unique_source_countries.csv.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afghanistan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angola</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>australia</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bahrain</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bangladesh</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>united_states</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>unknown</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>vietnam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>zambia</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          country  count\n",
       "0     afghanistan      1\n",
       "1          angola      1\n",
       "2       australia     12\n",
       "3         bahrain      1\n",
       "4      bangladesh      5\n",
       "..            ...    ...\n",
       "64  united_states     19\n",
       "65        unknown      9\n",
       "66        vietnam      1\n",
       "67         zambia      3\n",
       "68       zimbabwe      3\n",
       "\n",
       "[69 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "filepath = \"./processed_data/m_and_a_main_dataset.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# Get unique source country names and count of each\n",
    "unique_source_countries = df[\"source_country\"].value_counts().reset_index()\n",
    "\n",
    "# Rename columns\n",
    "unique_source_countries.columns = [\"country\", \"count\"]\n",
    "\n",
    "# Sort new df\n",
    "unique_source_countries = unique_source_countries.sort_values(by=\"country\").reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "output_filepath = \"./audit_data/m_and_a_unique_source_countries.csv\"\n",
    "unique_source_countries.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"File saved to {output_filepath}.\")\n",
    "# Display results\n",
    "unique_source_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m_and_a_unique_source_countries.csv was used to create a m_and_a_source_country_name_conversions.csv file in order to standardize country names according to the formatting used in this replication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize source_country names according to name conversions spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized dataset saved to ./processed_data/m_and_a_main_dataset.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "m_and_a_data = pd.read_csv(\"./processed_data/m_and_a_main_dataset.csv\")\n",
    "name_conversions = pd.read_csv(\n",
    "    \"./audit_data/m_and_a_source_country_name_conversion.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "# Function to standardize country names and convert to snake_case\n",
    "def standardize_country_names(data_df, conversion_df):\n",
    "    # Apply name mapping\n",
    "    name_map = conversion_df.set_index(\"old_name\")[\"new_name\"].dropna().to_dict()\n",
    "    data_df[\"source_country\"] = data_df[\"source_country\"].replace(name_map)\n",
    "\n",
    "    # Convert names to snake_case\n",
    "\n",
    "    data_df[\"source_country\"] = (\n",
    "        data_df[\"source_country\"]\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\", regex=False)\n",
    "        .str.replace(\"-\", \"_\", regex=False)\n",
    "        .str.replace(\",\", \"_\", regex=False)\n",
    "        .str.replace(\"'\", \"_\", regex=False)\n",
    "    )\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "# Apply the name conversion function to the M&A DataFrame\n",
    "m_and_a_data = standardize_country_names(m_and_a_data, name_conversions)\n",
    "\n",
    "# Save the filtered data\n",
    "output_filepath = \"./processed_data/m_and_a_main_dataset.csv\"\n",
    "m_and_a_data.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"Standardized dataset saved to {output_filepath}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out all instances where source_country == unknown or where source country == destination_country (i.e., non-FDI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances where source_country == unknown: 9\n",
      "Number of instances where source_country == destination_country: 68\n",
      "Number of rows in filtered dataset: 226\n",
      "Filtered dataset saved as ./processed_data/m_and_a_main_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "main_dataset_filepath = \"./processed_data/m_and_a_main_dataset.csv\"\n",
    "df = pd.read_csv(main_dataset_filepath)\n",
    "\n",
    "# County instances where source_country == \"unknown\"\n",
    "unknown_source_count = df[df[\"source_country\"] == \"unknown\"].shape[0]\n",
    "\n",
    "# Count instances where source_country == destination_country\n",
    "non_fdi_count = df[df[\"source_country\"] == df[\"destination_country\"]].shape[0]\n",
    "\n",
    "# Filter out non-FDI rows and rows where source_country == \"unknown\"\n",
    "filtered_dataset = df[\n",
    "    (df[\"source_country\"] != df[\"destination_country\"])\n",
    "    & (df[\"source_country\"] != \"unknown\")\n",
    "]\n",
    "\n",
    "# Update main dataset with filtered data\n",
    "\n",
    "filtered_dataset.to_csv(main_dataset_filepath, index=False)\n",
    "\n",
    "# Output results\n",
    "print(f\"Number of instances where source_country == unknown: {unknown_source_count}\")\n",
    "print(\n",
    "    f\"Number of instances where source_country == destination_country: {non_fdi_count}\"\n",
    ")\n",
    "print(f\"Number of rows in filtered dataset: {filtered_dataset.shape[0]}\")\n",
    "print(f\"Filtered dataset saved as {main_dataset_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create spreadsheet of remaning unique source countries for data auditing/validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>australia</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bahrain</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>british_virgin_islands</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cameroon</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>canada</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cayman_islands</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>china</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>congo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cote_d_ivoire</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>denmark</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ethiopia</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>fiji</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>france</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>germany</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ghana</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hong_kong</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>india</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>indonesia</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>israel</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>japan</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>jersey</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>kenya</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>kuwait</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>luxembourg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>malaysia</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>mauritius</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>morocco</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>netherlands</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>nigeria</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>norway</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>qatar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>russia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>saudi_arabia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>senegal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>singapore</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>south_africa</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>south_korea</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>spain</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>sudan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>switzerland</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>taiwan</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>thailand</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>united_arab_emirates</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>united_kingdom</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>united_states</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>vietnam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   country  count\n",
       "0                australia     12\n",
       "1                  bahrain      1\n",
       "2   british_virgin_islands      1\n",
       "3                 cameroon      2\n",
       "4                   canada     10\n",
       "5           cayman_islands      1\n",
       "6                    china     12\n",
       "7                    congo      1\n",
       "8            cote_d_ivoire      2\n",
       "9                  denmark      2\n",
       "10                ethiopia      4\n",
       "11                    fiji      1\n",
       "12                  france     17\n",
       "13                 germany      5\n",
       "14                   ghana      1\n",
       "15               hong_kong      6\n",
       "16                   india      5\n",
       "17               indonesia      3\n",
       "18                  israel      3\n",
       "19                   japan     16\n",
       "20                  jersey      1\n",
       "21                   kenya      7\n",
       "22                  kuwait      1\n",
       "23              luxembourg      2\n",
       "24                malaysia      5\n",
       "25               mauritius      3\n",
       "26                 morocco      3\n",
       "27             netherlands      6\n",
       "28                 nigeria      1\n",
       "29                  norway      1\n",
       "30                   qatar      1\n",
       "31                  russia      1\n",
       "32            saudi_arabia      1\n",
       "33                 senegal      1\n",
       "34               singapore     13\n",
       "35            south_africa     12\n",
       "36             south_korea      5\n",
       "37                   spain      1\n",
       "38                   sudan      1\n",
       "39             switzerland      5\n",
       "40                  taiwan      2\n",
       "41                thailand      4\n",
       "42    united_arab_emirates      3\n",
       "43          united_kingdom     18\n",
       "44           united_states     19\n",
       "45                 vietnam      1\n",
       "46                zimbabwe      3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load processed M&A dataset\n",
    "filepath = \"./processed_data/m_and_a_main_dataset.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# Get unique source country names and count of each\n",
    "unique_source_countries = df[\"source_country\"].value_counts().reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "unique_source_countries.columns = [\"country\", \"count\"]\n",
    "\n",
    "# Sort df\n",
    "unique_source_countries = unique_source_countries.sort_values(by=\"country\").reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "output_filepath = \"./audit_data/m_and_a_unique_source_countries.csv\"\n",
    "unique_source_countries.to_csv(output_filepath, index=False)\n",
    "\n",
    "# Display results\n",
    "unique_source_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare list of source countries with UNCTAD 2017-2018 developed country list for data audit/validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developed countries not appearing as source countries in m_and_a_main_dataset.csv:\n",
      "andorra\n",
      "austria\n",
      "belgium\n",
      "bermuda\n",
      "bulgaria\n",
      "croatia\n",
      "cyprus\n",
      "czechia\n",
      "estonia\n",
      "faroe_islands\n",
      "finland\n",
      "gibraltar\n",
      "greece\n",
      "greenland\n",
      "holy_see\n",
      "hungary\n",
      "iceland\n",
      "ireland\n",
      "italy\n",
      "latvia\n",
      "lithuania\n",
      "malta\n",
      "new_zealand\n",
      "poland\n",
      "portugal\n",
      "romania\n",
      "saint_pierre_and_miquelon\n",
      "san_marino\n",
      "slovakia\n",
      "slovenia\n",
      "sweden\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both spreadsheets into DataFrames\n",
    "source_countries_df = pd.read_csv(\"./audit_data/m_and_a_unique_source_countries.csv\")\n",
    "developed_countries_df = pd.read_csv(\n",
    "    \"./variable_data/developed_countries_2017_2018.csv\"\n",
    ")\n",
    "\n",
    "source_countries_set = set(source_countries_df[\"country\"])\n",
    "developed_countries_set = set(developed_countries_df[\"country\"])\n",
    "\n",
    "missing_in_source_countries = developed_countries_set - source_countries_set\n",
    "\n",
    "# Display results\n",
    "print(\n",
    "    \"Developed countries not appearing as source countries in m_and_a_main_dataset.csv:\"\n",
    ")\n",
    "for country in sorted(missing_in_source_countries):\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developed countries not appearing in m_and_a_main_dataset were checked manually against unique_source_countries.csv to ensure no matches were missed due to spelling or punctuation issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare list of countries in tax_havens.csv with countries appearing in source_country column and identify any tax_haven countries not appearing as source countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tax haven countries not appearing as source countries in m_and_a_main_dataset.csv:\n",
      "andorra\n",
      "anguilla\n",
      "antigua_and_barbuda\n",
      "aruba\n",
      "bahamas\n",
      "barbados\n",
      "belgium\n",
      "belize\n",
      "bermuda\n",
      "cyprus\n",
      "gibraltar\n",
      "grenada\n",
      "guernsey\n",
      "ireland\n",
      "isle_of_man\n",
      "lebanon\n",
      "liechtenstein\n",
      "macau\n",
      "malta\n",
      "marshall_islands\n",
      "monaco\n",
      "netherlands_antilles\n",
      "panama\n",
      "puerto_rico\n",
      "samoa\n",
      "seychelles\n",
      "st_kitts_and_nevis\n",
      "st_lucia\n",
      "st_vincent_and_grenadines\n",
      "turks_and_caicos\n",
      "vanuatu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both datasets as sets\n",
    "source_countries_set = set(\n",
    "    pd.read_csv(\"./audit_data/m_and_a_unique_source_countries.csv\")[\"country\"]\n",
    ")\n",
    "tax_havens_set = set(pd.read_csv(\"./variable_data/tax_havens.csv\")[\"country\"])\n",
    "\n",
    "# Create set of missing countries\n",
    "missing_tax_havens_set = tax_havens_set - source_countries_set\n",
    "\n",
    "# Display results\n",
    "print(\n",
    "    \"Tax haven countries not appearing as source countries in m_and_a_main_dataset.csv:\"\n",
    ")\n",
    "\n",
    "for country in sorted(missing_tax_havens_set):\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tax haven countries not appearing as source countries were manually checked against m_and_a_unique_source_countries.csv to ensure no matches were missed due to spelling or punctuation issues. No issues were found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create source_is_th variable. Value will be True if source_country appears in list of tax_havens and False otherwise. Tax havens list is taken from footnote 10 on page 1506 of Tørsløv, T., Wier, L., & Zucman, G. (2023). The missing profits of nations. The Review of Economic Studies, 90(3), 1499-1534."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m_and_a_main_dataset.csv updated with 'source_is_th' variable.\n",
      "Number of observations where 'source_is_th' is True: 39\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "m_and_a_df = pd.read_csv(\"./processed_data/m_and_a_main_dataset.csv\")\n",
    "tax_havens_df = pd.read_csv(\"./variable_data/tax_havens.csv\")\n",
    "\n",
    "# Create tax havens set\n",
    "tax_havens_set = set(tax_havens_df[\"country\"])\n",
    "\n",
    "# Add \"source_is_th\" variable\n",
    "m_and_a_df[\"source_is_th\"] = m_and_a_df[\"source_country\"].isin(tax_havens_set)\n",
    "\n",
    "# Save updated DataFrame/spreadsheet\n",
    "m_and_a_df.to_csv(\"./processed_data/m_and_a_main_dataset.csv\", index=False)\n",
    "print(\"m_and_a_main_dataset.csv updated with 'source_is_th' variable.\")\n",
    "\n",
    "# Outpute the number of observations where \"source_is_th\" is True to console\n",
    "true_count = m_and_a_df[\"source_is_th\"].sum()\n",
    "print(f\"Number of observations where 'source_is_th' is True: {true_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create destination_is_th variable. Value will be True if destination_country appears in list of tax_havens and False otherwise. Tax havens list taken from footnote 10 on page 1506 of Tørsløv, T., Wier, L., & Zucman, G. (2023). The missing profits of nations. The Review of Economic Studies, 90(3), 1499-1534."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m_and_a_df updated with 'destination_is_th' variable.\n",
      "Number of observations where \"destination_is_th\" is True: 2\n",
      "Countries where 'destination_is_th' is True:\n",
      "14     vanuatu\n",
      "109    vanuatu\n",
      "Name: destination_country, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the spreadsheets into DataFrames\n",
    "m_and_a_df = pd.read_csv(\"./processed_data/m_and_a_main_dataset.csv\")\n",
    "tax_havens_df = pd.read_csv(\"./variable_data/tax_havens.csv\")\n",
    "\n",
    "# Create tax havens set\n",
    "tax_havens_set = set(tax_havens_df[\"country\"])\n",
    "\n",
    "# Add \"destination_is_th\" variable\n",
    "m_and_a_df[\"destination_is_th\"] = m_and_a_df[\"destination_country\"].isin(tax_havens_set)\n",
    "\n",
    "# Save updated dataset\n",
    "m_and_a_df.to_csv(\"./processed_data/m_and_a_main_dataset.csv\", index=False)\n",
    "print(\"m_and_a_df updated with 'destination_is_th' variable.\")\n",
    "\n",
    "# Output the number of observations where \"destination_is_th\" is True to console\n",
    "true_count = m_and_a_df[\"destination_is_th\"].sum()\n",
    "print(f'Number of observations where \"destination_is_th\" is True: {true_count}')\n",
    "\n",
    "# Out put unique destination_country values where \"destination_is_th\" is True\n",
    "true_countries = m_and_a_df.loc[m_and_a_df[\"destination_is_th\"], \"destination_country\"]\n",
    "print(\"Countries where 'destination_is_th' is True:\")\n",
    "print(true_countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset comprising all observations where source_country is a tax haven and manually check accuracy as true origin of FDI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to m_and_a_source_is_th_unique_csv\n",
      "Number of unique parent companies in filtered data: 36\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "m_and_a_df = pd.read_csv(\"./processed_data/m_and_a_main_dataset.csv\")\n",
    "\n",
    "# Fitler for instances where source_is_th == True\n",
    "m_and_a_source_is_th_df = m_and_a_df[m_and_a_df[\"source_is_th\"] == True]\n",
    "\n",
    "# Select the specific columns\n",
    "selected_columns = [\n",
    "    \"master_deal_no\",\n",
    "    \"tmanames\",\n",
    "    \"amanames\",\n",
    "    \"aupnames\",\n",
    "    \"source_country\",\n",
    "    \"destination_country\",\n",
    "    \"abusinessl\",\n",
    "    \"tbusinessl\",\n",
    "    \"atf_mid_desc\",\n",
    "    \"ttf_mid_desc\",\n",
    "    \"atf_macro_desc\",\n",
    "    \"ttf_macro_desc\",\n",
    "]\n",
    "\n",
    "# Retain only unique rows based on 'parent company' and selected columns\n",
    "unique_parent_company_df = m_and_a_source_is_th_df[selected_columns].drop_duplicates(\n",
    "    subset=[\"amanames\"]\n",
    ")\n",
    "\n",
    "# Convert new dataframe to CSV for manual checking\n",
    "unique_parent_company_df.to_csv(\n",
    "    \"./audit_data/m_and_a_source_is_th_unique.csv\", index=False\n",
    ")\n",
    "\n",
    "num_observations = unique_parent_company_df.shape[0]\n",
    "\n",
    "print(\"Filtered data saved to m_and_a_source_is_th_unique_csv\")\n",
    "print(f\"Number of unique parent companies in filtered data: {num_observations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually check all companies to determine accuracy of parent country. Results/comments saved to \"./audit_data/m_and_a_source_th_man_flagged.csv\". Companies with source countries identified as tax havens were checked manually on S&P Capital IQ, Orbis, and on company websites (where available). In instances where the stated source country seemed to be either incorrect, or seemed questionable, source_country_questionable is flagged as TRUE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new dataset containing only parent companies whre source country is questionable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to ./audit_data/m_and_a_source_questionable.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "filepath = \"./audit_data/m_and_a_source_th_man_flagged.csv\"\n",
    "data = pd.read_csv(filepath)\n",
    "\n",
    "# Convert the 'source_country_questionable' columh to boolean\n",
    "data[\"source_country_questionable\"] = (\n",
    "    data[\"source_country_questionable\"].str.strip().str.upper() == \"TRUE\"\n",
    ")\n",
    "\n",
    "# Filter rows where 'source_country_questionable' is True\n",
    "filtered_data = data[data[\"source_country_questionable\"] == True]\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "filtered_filepath = \"./audit_data/m_and_a_source_questionable.csv\"\n",
    "filtered_data.to_csv(filtered_filepath, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to {filtered_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use flagged data to create source_questionable in m_and_a_main_dataset.csv. Value is True if the source country has been flagged as questionable in the m_and_a_source_questionable_csv and false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated datset saved to ./processed_data/m_and_a_main_dataset.csv\n",
      "Number of observations with source_questionable = True: 21\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "main_dataset_filepath = \"./processed_data/m_and_a_main_dataset.csv\"\n",
    "questionable_source_filepath = \"./audit_data/m_and_a_source_questionable.csv\"\n",
    "\n",
    "main_dataset = pd.read_csv(main_dataset_filepath)\n",
    "questionable_source = pd.read_csv(questionable_source_filepath)\n",
    "\n",
    "# Extract list of parent companies from questionable source dataset\n",
    "questionable_parent_companies = questionable_source[\"aupnames\"].unique()\n",
    "\n",
    "# Create the \"source_questionable\" column in main dataset\n",
    "main_dataset[\"source_questionable\"] = main_dataset[\"aupnames\"].isin(\n",
    "    questionable_parent_companies\n",
    ")\n",
    "\n",
    "# Save the updated dataset to CSV\n",
    "main_dataset.to_csv(main_dataset_filepath, index=False)\n",
    "\n",
    "print(f\"Updated datset saved to {main_dataset_filepath}\")\n",
    "\n",
    "true_count = main_dataset[\"source_questionable\"].sum()\n",
    "print(f\"Number of observations with source_questionable = True: {true_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22 observations flagged in the M&A dataset where the source country is questionable after manual checking of all instances where \"source_country\" is a tax haven (see above re: list of tax haven countries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"developed\" variable in greenfield_main_dataset.csv. Variable takes value of True if source country appears in list of developed countries and False otherwise. Also created \"emne\" variable that takes the inverse value of the \"developed\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m_and_a_main_dataset.csv updated with \"developed\" and \"emne\" variables.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load spreadsheets into DataFrames\n",
    "\n",
    "m_and_a_filepath = \"./processed_data/m_and_a_main_dataset.csv\"\n",
    "developed_countries_filepath = \"./variable_data/developed_countries_2017_2018.csv\"\n",
    "\n",
    "m_and_a_df = pd.read_csv(m_and_a_filepath)\n",
    "developed_countries_df = pd.read_csv(developed_countries_filepath)\n",
    "\n",
    "# Create set of developed countries\n",
    "developed_countries_set = set(developed_countries_df[\"country\"])\n",
    "\n",
    "# Add \"developed\" variable to m_and_a_df\n",
    "m_and_a_df[\"developed\"] = m_and_a_df[\"source_country\"].isin(developed_countries_set)\n",
    "\n",
    "# Add \"emne\" variable to m_and_a_df\n",
    "m_and_a_df[\"emne\"] = ~m_and_a_df[\"source_country\"].isin(developed_countries_set)\n",
    "\n",
    "# Save the updated DataFrame as m_and_a_main_dataset.csv\n",
    "m_and_a_df.to_csv(m_and_a_filepath, index=False)\n",
    "\n",
    "print(f'm_and_a_main_dataset.csv updated with \"developed\" and \"emne\" variables.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"bordering_country\" variable. Variable is True if source countries shares a border with destination country and False otherwise. Bordering countries checked from CIA World Factbook https://www.cia.gov/the-world-factbook/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new variable \"bordering_country\" has been added and the updated file has been saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load spreadsheets into DataFrames\n",
    "m_and_a_filepath = \"./processed_data/m_and_a_main_dataset.csv\"\n",
    "bordering_countries_filepath = \"./variable_data/bordering_countries_2017_2018.csv\"\n",
    "\n",
    "m_and_a_df = pd.read_csv(m_and_a_filepath)\n",
    "bordering_countries_df = pd.read_csv(bordering_countries_filepath)\n",
    "\n",
    "# Create dictionary of bordering countries\n",
    "bordering_countries_dict = bordering_countries_df.set_index(\"ldc\")[\n",
    "    \"bordering_countries\"\n",
    "].to_dict()\n",
    "\n",
    "# Function to check if the source country is in the list of bordering countries\n",
    "def is_bordering(source_country, destination_country):\n",
    "    bordering_countries = bordering_countries_dict.get(destination_country, \"\")\n",
    "    if pd.isna(bordering_countries) or not bordering_countries:\n",
    "        return False\n",
    "    bordering_list = bordering_countries.split(\", \")\n",
    "    return source_country in bordering_list\n",
    "\n",
    "# Apply the is_bordering function to create the new \"bordering_country\" in m_and_a_df\n",
    "m_and_a_df[\"bordering_country\"] = m_and_a_df.apply(\n",
    "    lambda row: is_bordering(row[\"source_country\"], row[\"destination_country\"]), axis=1\n",
    ")\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "m_and_a_df.to_csv(m_and_a_filepath, index=False)\n",
    "\n",
    "print(\n",
    "    f'The new variable \"bordering_country\" has been added and the updated file has been saved'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new \"industry\" variable by based on ttf_mid_desc variable. The Refinitiv manual for the M&A database defines ttf_mid_desc as: Target Mid Industry (Code): Refinitiv’s proprietary\n",
    "mid-level industry classifications based on SIC\n",
    "Codes, NAIC Codes and overall company business\n",
    "description. There are more than 85 mid-level\n",
    "industry classifications grouped by 14 macro-level\n",
    "categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset has been saved to ./processed_data/m_and_a_main_dataset.csv.\n",
      "        Industry counts has been saved to ./audit_data/m_and_industry_counts.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load processed M&A dataset\n",
    "filepath = \"./processed_data/m_and_a_main_dataset.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# Define function to convert strings to snake_case\n",
    "def to_snake_case(s):\n",
    "    s = (\n",
    "        s.replace(\" \", \"_\")\n",
    "        .replace(\"'\", \"_\")\n",
    "        .replace(\",\", \"_\")\n",
    "        .replace(\"/\", \"_\")\n",
    "        .replace(\"-\", \"_\")\n",
    "        .replace(\"(\", \"_\")\n",
    "        .replace(\")\", \"\")\n",
    "        .replace(\" / \", \"_\")\n",
    "        .replace(\"&\", \"and\")\n",
    "    )\n",
    "\n",
    "    return s.lower()\n",
    "\n",
    "# Create new 'industry' column that is 'ttf_mid_desc' converted to snake_case\n",
    "df[\"industry\"] = df[\"ttf_mid_desc\"].apply(to_snake_case)\n",
    "\n",
    "# Save the modified DataFrame to csv\n",
    "df.to_csv(filepath, index=False)\n",
    "\n",
    "# Calculate the count of unique values in 'industry' column\n",
    "industry_counts = df[\"industry\"].value_counts().reset_index()\n",
    "industry_counts.columns = [\"industry\", \"count\"]\n",
    "\n",
    "# Sort alphabetically\n",
    "industry_counts.sort_values(by=\"industry\", inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "industry_counts_filepath = \"./audit_data/m_and_industry_counts.csv\"\n",
    "industry_counts.to_csv(industry_counts_filepath, index=False)\n",
    "\n",
    "print(\n",
    "    f\"Updated dataset has been saved to {filepath}.\\n\\\n",
    "        Industry counts has been saved to {industry_counts_filepath}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create natural_resource variable. Variable is True is industry is listed as a natural resource industry in the nat_resource_list.csv spreadsheet. Industries flagged as natural resource industries are:\n",
    "- agriculture_and_livestock\n",
    "- metals_and_mining\n",
    "- oil_and_gas\n",
    "- paper_and_forest_products\n",
    "- tobacco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The modified dataset has been saved to ./processed_data/m_and_a_main_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both spreadsheets into DataFrames\n",
    "main_dataset_path = \"./processed_data/m_and_a_main_dataset.csv\"\n",
    "nat_resource_list_path = \"./variable_data/m_and_a_nat_resource_list.csv\"\n",
    "\n",
    "main_df = pd.read_csv(main_dataset_path)\n",
    "nat_resource_df = pd.read_csv(nat_resource_list_path)\n",
    "\n",
    "# Create a set of natural resource industries\n",
    "natural_resource_set = set(\n",
    "    nat_resource_df[nat_resource_df[\"nat_resource\"] == True][\"industry\"]\n",
    ")\n",
    "\n",
    "# Create a new column \"natural_resource_ind\" in main_df\n",
    "main_df[\"natural_resource_ind\"] = main_df[\"industry\"].apply(\n",
    "    lambda x: x in natural_resource_set\n",
    ")\n",
    "\n",
    "# Save the modified DataFrame to CSV\n",
    "main_df.to_csv(main_dataset_path, index=False)\n",
    "\n",
    "print(f\"The modified dataset has been saved to {main_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"colonial_link\" variable. Variable is True if the source country is listed in colonial_rulers.csv\" and False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset saved to ./processed_data/m_and_a_main_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both spreadsheets into DataFrames\n",
    "main_dataset_path = \"./processed_data/m_and_a_main_dataset.csv\"\n",
    "colonial_rulers_path = \"./variable_data/colonial_rulers_list.csv\"\n",
    "\n",
    "main_df = pd.read_csv(main_dataset_path)\n",
    "colonial_rulers_df = pd.read_csv(colonial_rulers_path)\n",
    "\n",
    "# Create a dictionary from colonial_rulters_list\n",
    "colonial_rulers_dict = colonial_rulers_df.set_index(\"country\")[\n",
    "    \"colonial_rulers\"\n",
    "    ].to_dict()\n",
    "\n",
    "# Function to check for colonial link\n",
    "def has_colonial_link(row):\n",
    "    destination = row[\"destination_country\"]\n",
    "    source = row[\"source_country\"]\n",
    "    colonial_rulers = colonial_rulers_dict.get(destination)\n",
    "\n",
    "    if isinstance(colonial_rulers, str):\n",
    "        colonial_rulers_list = [\n",
    "            ruler.strip().replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"'\", \"_\").lower()\n",
    "            for ruler in colonial_rulers.split(\", \")\n",
    "        ]\n",
    "        return source in colonial_rulers_list\n",
    "    return False\n",
    "\n",
    "# Apply the function to create new \"colonial_link\" column\n",
    "main_df[\"colonial_link\"] = main_df.apply(has_colonial_link, axis=1)\n",
    "\n",
    "# Save the updated DataFrame to CSV\n",
    "main_df.to_csv(main_dataset_path, index=False)\n",
    "\n",
    "print(f\"Updated dataset saved to {main_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 2017 and 2018 spreadsheets for each individual LDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country spreadsheets created and saved to \"country_spreadsheets/m_and_a\"\n",
      "\n",
      "Country/Year pairs with no data:\n",
      "- Country: benin, Year: 2018\n",
      "- Country: bhutan, Year: 2017\n",
      "- Country: burundi, Year: 2017\n",
      "- Country: comoros, Year: 2017\n",
      "- Country: comoros, Year: 2018\n",
      "- Country: djibouti, Year: 2017\n",
      "- Country: east_timor, Year: 2017\n",
      "- Country: east_timor, Year: 2018\n",
      "- Country: eritrea, Year: 2017\n",
      "- Country: gambia, Year: 2017\n",
      "- Country: gambia, Year: 2018\n",
      "- Country: guinea, Year: 2018\n",
      "- Country: guinea_bissau, Year: 2017\n",
      "- Country: guinea_bissau, Year: 2018\n",
      "- Country: haiti, Year: 2017\n",
      "- Country: haiti, Year: 2018\n",
      "- Country: kiribati, Year: 2017\n",
      "- Country: kiribati, Year: 2018\n",
      "- Country: liberia, Year: 2017\n",
      "- Country: liberia, Year: 2018\n",
      "- Country: mauritania, Year: 2017\n",
      "- Country: nepal, Year: 2017\n",
      "- Country: niger, Year: 2017\n",
      "- Country: sao_tome_and_principe, Year: 2017\n",
      "- Country: sao_tome_and_principe, Year: 2018\n",
      "- Country: sierra_leone, Year: 2018\n",
      "- Country: solomon_islands, Year: 2017\n",
      "- Country: solomon_islands, Year: 2018\n",
      "- Country: somalia, Year: 2017\n",
      "- Country: somalia, Year: 2018\n",
      "- Country: south_sudan, Year: 2017\n",
      "- Country: south_sudan, Year: 2018\n",
      "- Country: tuvalu, Year: 2017\n",
      "- Country: tuvalu, Year: 2018\n",
      "- Country: yemen, Year: 2018\n",
      "\n",
      "Countries with no data in either year:\n",
      "- Country: comoros\n",
      "- Country: east_timor\n",
      "- Country: gambia\n",
      "- Country: guinea_bissau\n",
      "- Country: haiti\n",
      "- Country: kiribati\n",
      "- Country: liberia\n",
      "- Country: sao_tome_and_principe\n",
      "- Country: solomon_islands\n",
      "- Country: somalia\n",
      "- Country: south_sudan\n",
      "- Country: tuvalu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load both spreadsheets into DataFrames\n",
    "main_dataset_path = \"./processed_data/m_and_a_main_dataset.csv\"\n",
    "ldcs_list_path = \"./variable_data/ldcs_list_2017_2018.csv\"\n",
    "\n",
    "main_df = pd.read_csv(main_dataset_path)\n",
    "ldcs_df = pd.read_csv(ldcs_list_path)\n",
    "\n",
    "\n",
    "# Define function to conver country names to snake_case\n",
    "def to_snake_case(s):\n",
    "    return s.strip().replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"'\", \"_\").lower()\n",
    "\n",
    "\n",
    "# Convert country names in both DataFrames to snake_case\n",
    "main_df[\"destination_country\"] = main_df[\"destination_country\"].apply(to_snake_case)\n",
    "ldcs_df[\"country\"] = ldcs_df[\"country\"].apply(to_snake_case)\n",
    "\n",
    "# Convert \"dateann\" to datetime for easier filtering\n",
    "main_df[\"dateann\"] = pd.to_datetime(main_df[\"dateann\"], errors=\"coerce\")\n",
    "\n",
    "# Extract year from \"dateann\"\n",
    "main_df[\"year\"] = main_df[\"dateann\"].dt.year\n",
    "\n",
    "# Create output directories for country spreadsheets\n",
    "output_dir = \"./country_spreadsheets/m_and_a\"\n",
    "\n",
    "for year in [2017, 2018]:\n",
    "    year_dir = os.path.join(output_dir, str(year))\n",
    "    if not os.path.exists(year_dir):\n",
    "        os.makedirs(year_dir)\n",
    "\n",
    "# Create list to keep track of rows not assigned to a spreadsheet\n",
    "unassigned_rows = []\n",
    "\n",
    "# List to keep track of empty spreadsheets\n",
    "empty_spreadsheets = []\n",
    "\n",
    "# List to keep track of country/year pairs with no data\n",
    "no_data_pairs = []\n",
    "\n",
    "# List to keep track of countries with no data in either year\n",
    "no_data_countries = []\n",
    "\n",
    "# Process each country in the ldcs_df\n",
    "for country in ldcs_df[\"country\"]:\n",
    "    country_has_data = False # Flag to track i a country has any data\n",
    "\n",
    "    for year in [2017, 2018]:\n",
    "        # Filter data for the specific country and year\n",
    "        country_year_data = main_df[\n",
    "            (main_df[\"destination_country\"] == country) & (main_df[\"year\"] == year)\n",
    "        ]\n",
    "\n",
    "        # Create filename\n",
    "        filename = f\"{output_dir}/{year}/{country}_m_and_a_{year}.csv\"\n",
    "\n",
    "        # Save data to CSV\n",
    "        if not country_year_data.empty:\n",
    "            country_year_data.to_csv(filename, index=False)\n",
    "            country_has_data = True # Mark that this country has data\n",
    "        else:\n",
    "            # Save an empty CSV with the same headers as m_and_a_main_dataset.csv\n",
    "            empty_df = main_df.head(0)\n",
    "            empty_df.to_csv(filename, index=False)\n",
    "            empty_spreadsheets.append(filename)\n",
    "            # Track the no-data pair\n",
    "            no_data_pairs.append((country, year))\n",
    "\n",
    "    # if the country had no data in either year, add it to no_data_countries\n",
    "    if not country_has_data:\n",
    "        no_data_countries.append(country)\n",
    "\n",
    "# Identify rows not assigned to any country spreadsheet\n",
    "assigned_rows = main_df[main_df[\"destination_country\"].isin(ldcs_df[\"country\"])]\n",
    "unassigned_rows = main_df[~main_df.index.isin(assigned_rows.index)]\n",
    "\n",
    "# Save unassigned rows to a CSV file\n",
    "audit_dir = \"./audit_data\"\n",
    "if not os.path.exists(audit_dir):\n",
    "    os.makedirs(audit_dir)\n",
    "\n",
    "unassigned_filename = f\"{audit_dir}/unassigned_rows_m_and_a.csv\"\n",
    "unassigned_rows.to_csv(unassigned_filename, index=False)\n",
    "\n",
    "# Save the list of empty spreadsheets to a CSV file\n",
    "empty_spreadsheets_filename = f\"{audit_dir}/empty_spreadsheets_m_and_a.csv\"\n",
    "pd.DataFrame(empty_spreadsheets, columns=[\"empty_spreadsheets\"]).to_csv(\n",
    "    empty_spreadsheets_filename, index=False\n",
    ")\n",
    "\n",
    "print('Country spreadsheets created and saved to \"country_spreadsheets/m_and_a\"')\n",
    "\n",
    "# Print country/year pairs with no data\n",
    "if no_data_pairs:\n",
    "    print(\"\\nCountry/Year pairs with no data:\")\n",
    "    for country, year in no_data_pairs:\n",
    "        print(f\"- Country: {country}, Year: {year}\")\n",
    "\n",
    "# Print countries with no data in either year\n",
    "if no_data_countries:\n",
    "    print(\"\\nCountries with no data in either year:\")\n",
    "    for country in no_data_countries:\n",
    "        print(f\"- Country: {country}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of countries with no spreadsheets corresponds to earlier list of countries not appearing in filtered M&A dataset destination_country column (see above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate percentage of deals for which there is no deal_value given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of deals with a value: 90\n",
      "Number of deals with no value: 136\n",
      "Percentage of deals with a value: 39.823008849557525\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "filepath = \"./processed_data/m_and_a_main_dataset.csv\"\n",
    "data = pd.read_csv(filepath)\n",
    "\n",
    "if \"deal_value\" in data.columns:\n",
    "    # Number of deals with a value under \"deal_value\"\n",
    "    deals_with_value = data[\"deal_value\"].notnull().sum()\n",
    "\n",
    "    # Number of deals with no value under \"deal_value\"\n",
    "    deals_without_value = data[\"deal_value\"].isnull().sum()\n",
    "\n",
    "    # Percentage of deals with a value under \"deal_value\"\n",
    "    total_deals = len(data)\n",
    "    percentage_with_value = (deals_with_value / total_deals) * 100\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Number of deals with a value: {deals_with_value}\")\n",
    "    print(f\"Number of deals with no value: {deals_without_value}\")\n",
    "    print(f\"Percentage of deals with a value: {percentage_with_value}\")\n",
    "else:\n",
    "    print(\"The 'deal_value' column is missing is in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dependent variables and two control variables (colonial_link and geographic_proximity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Situations in which variables should be None:\n",
    "\n",
    "<u>Count-Based Variables:</u>  \n",
    "None if:\n",
    "- There are no transactions (both datasets are empty).\n",
    "Valid if:\n",
    "- Count reflects valid rows, even if filtered subsets (e.g., EMNE) result in 0.\n",
    "\n",
    "<u>Proportion Variables:</u>  \n",
    "None if:\n",
    "- The denominator is None (e.g., total_fdi is invalid or missing).\n",
    "0.0 if:\n",
    "- The numerator is effectively None (e.g., no matching transactions), but the denominator is valid.\n",
    "Calculated normally if:\n",
    "- Both numerator and denominator are valid.\n",
    "General Rules:\n",
    "- None represents missing or invalid data.\n",
    "- 0 or 0.0 represents valid data where no matching transactions exist.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Datasets saved for 2017 and 2018.\n"
     ]
    }
   ],
   "source": [
    "# Streamlined version\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define base and output directories\n",
    "base_dir = \"./country_spreadsheets\"\n",
    "output_dir = \"./processed_data\"\n",
    "years = [\"2017\", \"2018\"]\n",
    "\n",
    "# Initialize dictionary to hold data for each year\n",
    "all_data = {year: [] for year in years}\n",
    "\n",
    "\n",
    "def process_data(filepath, data_type):\n",
    "    \"\"\"Process Greenfield or M&A data and return relevant variables.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"The file {filepath} does not exist.\")\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    if df.empty:  # Handle empty dataframes\n",
    "        return {\n",
    "            \"colonial_link_present\": None,\n",
    "            \"geographic_proximity_present\": None,\n",
    "            f\"{data_type}_count\": 0,\n",
    "            f\"{data_type}_emne_count\": 0,\n",
    "            f\"{data_type}_count_excl_nat_res\": 0,\n",
    "            f\"{data_type}_count_excl_col_link\": 0,\n",
    "            f\"{data_type}_count_excl_qst_source\": 0,\n",
    "            f\"{data_type}_emne_count_excl_nat_res\": 0,\n",
    "            f\"{data_type}_emne_count_excl_col_link\": 0,\n",
    "            f\"{data_type}_emne_count_excl_qst_source\": 0,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"colonial_link_present\": df[\"colonial_link\"].any(),\n",
    "        \"geographic_proximity_present\": df[\"bordering_country\"].any(),\n",
    "        f\"{data_type}_count\": len(df),\n",
    "        f\"{data_type}_emne_count\": len(df[df[\"emne\"]]),\n",
    "        f\"{data_type}_count_excl_nat_res\": len(df[~df[\"natural_resource_ind\"]]),\n",
    "        f\"{data_type}_count_excl_col_link\": len(df[~df[\"colonial_link\"]]),\n",
    "        f\"{data_type}_count_excl_qst_source\": len(df[~df[\"source_questionable\"]]),\n",
    "        f\"{data_type}_emne_count_excl_nat_res\": len(\n",
    "            df[(df[\"emne\"] & ~df[\"natural_resource_ind\"])]\n",
    "        ),\n",
    "        f\"{data_type}_emne_count_excl_col_link\": len(\n",
    "            df[(df[\"emne\"] & ~df[\"colonial_link\"])]\n",
    "        ),\n",
    "        f\"{data_type}_emne_count_excl_qst_source\": len(\n",
    "            df[(df[\"emne\"] & ~df[\"source_questionable\"])]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "# Process data for each year\n",
    "for year in years:\n",
    "    greenfield_dir = os.path.join(base_dir, \"greenfield\", year)\n",
    "    m_and_a_dir = os.path.join(base_dir, \"m_and_a\", year)\n",
    "\n",
    "    filenames = sorted(os.listdir(greenfield_dir))\n",
    "\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".csv\"):\n",
    "            country = \"_\".join(filename.split(\"_\")[:-2])\n",
    "            greenfield_filepath = os.path.join(greenfield_dir, filename)\n",
    "            m_and_a_filepath = os.path.join(\n",
    "                m_and_a_dir, filename.replace(\"greenfield\", \"m_and_a\")\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(m_and_a_filepath):\n",
    "                raise FileNotFoundError(f\"M&A file missing for {country} in {year}.\")\n",
    "\n",
    "            # Process data\n",
    "            greenfield_data = process_data(greenfield_filepath, \"greenfield\")\n",
    "            m_and_a_data = process_data(m_and_a_filepath, \"m_and_a\")\n",
    "\n",
    "            # Check if both datasets are empty\n",
    "            if all(\n",
    "                value == 0 or value is None for value in greenfield_data.values()\n",
    "            ) and all(value == 0 or value is None for value in m_and_a_data.values()):\n",
    "                combined_data = {\n",
    "                    \"country\": country,\n",
    "                    \"year\": year,\n",
    "                    \"prop_emne\": None,\n",
    "                    \"prop_emne_excl_nat_res\": None,\n",
    "                    \"prop_emne_excl_col_link\": None,\n",
    "                    \"prop_emne_excl_qst_source\": None,\n",
    "                    \"colonial_link\": None,\n",
    "                    \"geographic_proximity\": None,\n",
    "                }\n",
    "            else:\n",
    "                # Calculate combined proportions\n",
    "                combined_data = {\n",
    "                    \"country\": country,\n",
    "                    \"year\": year,\n",
    "                    \"prop_emne\": (\n",
    "                        (\n",
    "                            greenfield_data[\"greenfield_emne_count\"]\n",
    "                            + m_and_a_data[\"m_and_a_emne_count\"]\n",
    "                        )\n",
    "                        / (\n",
    "                            greenfield_data[\"greenfield_count\"]\n",
    "                            + m_and_a_data[\"m_and_a_count\"]\n",
    "                        )\n",
    "                        if (\n",
    "                            greenfield_data[\"greenfield_count\"]\n",
    "                            + m_and_a_data[\"m_and_a_count\"]\n",
    "                        )\n",
    "                        > 0\n",
    "                        else None\n",
    "                    ),\n",
    "                    \"prop_emne_excl_nat_res\": (\n",
    "                        (\n",
    "                            greenfield_data[\"greenfield_emne_count_excl_nat_res\"]\n",
    "                            + m_and_a_data[\"m_and_a_emne_count_excl_nat_res\"]\n",
    "                        )\n",
    "                        / (\n",
    "                            greenfield_data[\"greenfield_count_excl_nat_res\"]\n",
    "                            + m_and_a_data[\"m_and_a_count_excl_nat_res\"]\n",
    "                        )\n",
    "                        if (\n",
    "                            greenfield_data[\"greenfield_count_excl_nat_res\"]\n",
    "                            + m_and_a_data[\"m_and_a_count_excl_nat_res\"]\n",
    "                        )\n",
    "                        > 0\n",
    "                        else None\n",
    "                    ),\n",
    "                    \"prop_emne_excl_col_link\": (\n",
    "                        (\n",
    "                            greenfield_data[\"greenfield_emne_count_excl_col_link\"]\n",
    "                            + m_and_a_data[\"m_and_a_emne_count_excl_col_link\"]\n",
    "                        )\n",
    "                        / (\n",
    "                            greenfield_data[\"greenfield_count_excl_col_link\"]\n",
    "                            + m_and_a_data[\"m_and_a_count_excl_col_link\"]\n",
    "                        )\n",
    "                        if (\n",
    "                            greenfield_data[\"greenfield_count_excl_col_link\"]\n",
    "                            + m_and_a_data[\"m_and_a_count_excl_col_link\"]\n",
    "                        )\n",
    "                        > 0\n",
    "                        else None\n",
    "                    ),\n",
    "                    \"prop_emne_excl_qst_source\": (\n",
    "                        (\n",
    "                            greenfield_data[\"greenfield_emne_count_excl_qst_source\"]\n",
    "                            + m_and_a_data[\"m_and_a_emne_count_excl_qst_source\"]\n",
    "                        )\n",
    "                        / (\n",
    "                            greenfield_data[\"greenfield_count_excl_qst_source\"]\n",
    "                            + m_and_a_data[\"m_and_a_count_excl_qst_source\"]\n",
    "                        )\n",
    "                        if (\n",
    "                            greenfield_data[\"greenfield_count_excl_qst_source\"]\n",
    "                            + m_and_a_data[\"m_and_a_count_excl_qst_source\"]\n",
    "                        )\n",
    "                        > 0\n",
    "                        else None\n",
    "                    ),\n",
    "                    \"colonial_link\": greenfield_data[\"colonial_link_present\"]\n",
    "                    or m_and_a_data[\"colonial_link_present\"],\n",
    "                    \"geographic_proximity\": greenfield_data[\n",
    "                        \"geographic_proximity_present\"\n",
    "                    ]\n",
    "                    or m_and_a_data[\"geographic_proximity_present\"],\n",
    "                }\n",
    "\n",
    "            all_data[year].append(combined_data)\n",
    "\n",
    "# Save the results for each year\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for year, data in all_data.items():\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.sort_values(by=[\"year\", \"country\"])\n",
    "    df.to_csv(os.path.join(output_dir, f\"master_dataset_{year}.csv\"), index=False)\n",
    "\n",
    "print(\"Processing complete. Datasets saved for 2017 and 2018.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add governance variables and control variables, all of which can be downloaded for LDCs from World Bank DataBank here:\n",
    "https://databank.worldbank.org/source/world-development-indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data for 2017:\n",
      "         country  year                 variable\n",
      "423  afghanistan  2016           gni_per_capita\n",
      "425   bangladesh  2016           gni_per_capita\n",
      "51        bhutan  2016                prop_emne\n",
      "98        bhutan  2016   prop_emne_excl_nat_res\n",
      "145       bhutan  2016  prop_emne_excl_col_link\n",
      "..           ...   ...                      ...\n",
      "277       tuvalu  2016            colonial_link\n",
      "324       tuvalu  2016     geographic_proximity\n",
      "465       tuvalu  2016           gni_per_capita\n",
      "468        yemen  2016           gni_per_capita\n",
      "469       zambia  2016           gni_per_capita\n",
      "\n",
      "[92 rows x 3 columns]\n",
      "Missing data for 2018:\n",
      "         country  year                   variable\n",
      "423  afghanistan  2017             gni_per_capita\n",
      "425   bangladesh  2017             gni_per_capita\n",
      "57       comoros  2017                  prop_emne\n",
      "104      comoros  2017     prop_emne_excl_nat_res\n",
      "151      comoros  2017    prop_emne_excl_col_link\n",
      "..           ...   ...                        ...\n",
      "233        yemen  2017  prop_emne_excl_qst_source\n",
      "280        yemen  2017              colonial_link\n",
      "327        yemen  2017       geographic_proximity\n",
      "468        yemen  2017             gni_per_capita\n",
      "469       zambia  2017             gni_per_capita\n",
      "\n",
      "[84 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "world_bank_data = pd.read_csv(\"./raw_data/world_bank_data_2016_2017.csv\")\n",
    "master_dataset_2017 = pd.read_csv(\"./processed_data/master_dataset_2017.csv\")\n",
    "master_dataset_2018 = pd.read_csv(\"./processed_data/master_dataset_2018.csv\")\n",
    "world_bank_var_names = pd.read_csv(\"./audit_data/world_bank_var_names.csv\")\n",
    "name_conversions = pd.read_csv(\"./audit_data/world_bank_name_conversions.csv\")\n",
    "ldcs_list = pd.read_csv(\"./variable_data/ldcs_list_2017_2018.csv\")\n",
    "\n",
    "\n",
    "# Step 1: Standardize country names\n",
    "def standardize_country_names(df, conversion_df):\n",
    "    # Apply name mapping\n",
    "    name_map = conversion_df.set_index(\"old_name\")[\"new_name\"].dropna().to_dict()\n",
    "    df[\"country\"] = df[\"Country Name\"].replace(name_map)\n",
    "\n",
    "    # Then convert the names to snake_case\n",
    "    df[\"country\"] = (\n",
    "        df[\"country\"]\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\", regex=False)\n",
    "        .str.replace(\"-\", \"_\", regex=False)\n",
    "        .str.replace(\",\", \"_\", regex=False)\n",
    "        .str.replace(\"'\", \"_\", regex=False)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Replace \"..\" values in world_bank_data with NaN\n",
    "world_bank_data.replace(\"..\", pd.NA, inplace=True)\n",
    "\n",
    "# Apply the name conversion process\n",
    "world_bank_data = standardize_country_names(world_bank_data, name_conversions)\n",
    "\n",
    "# Step 2: Rename variables based on world_bank_var_names.csv and reshape data\n",
    "var_name_map = world_bank_var_names.set_index(\"old_name\")[\"new_name\"].to_dict()\n",
    "\n",
    "\n",
    "def rename_world_bank_variables(df, var_map):\n",
    "    df[\"Series Name\"] = df[\"Series Name\"].replace(var_map)\n",
    "    df = df.pivot(\n",
    "        index=\"country\",\n",
    "        columns=\"Series Name\",\n",
    "        values=[\"2016 [YR2016]\", \"2017 [YR2017]\"],\n",
    "    )\n",
    "    df.columns = [\n",
    "        f\"{col[1]}_{col[0][:4]}\" for col in df.columns.values\n",
    "    ]  # Format as, e.g. \"gni_per_capita_2016\"\n",
    "    return df.reset_index()\n",
    "\n",
    "\n",
    "# Rename variables and reshape data\n",
    "world_bank_data_renamed = rename_world_bank_variables(world_bank_data, var_name_map)\n",
    "\n",
    "\n",
    "# Step 3: Strip year from column names before merging\n",
    "def strip_year_from_columns(df, year):\n",
    "    \"\"\"\n",
    "    Rename columns to remove year suffix for a specific year.\n",
    "    E.g., 'gdp_per_capita_2016' -> 'gdp_per_capita'\n",
    "    \"\"\"\n",
    "    year_suffix = f\"_{year}\"\n",
    "    rename_map = {\n",
    "        col: col.replace(year_suffix, \"\")\n",
    "        for col in df.columns\n",
    "        if col.endswith(year_suffix)\n",
    "    }\n",
    "    return df.rename(columns=rename_map)\n",
    "\n",
    "\n",
    "# Step 4: Merge transformed World Bank data with the master datasets\n",
    "def merge_world_bank_data(master_df, wb_df, year_columns):\n",
    "    for col in year_columns:\n",
    "        # Check if the column already exists in the master dataset\n",
    "        if col in master_df.columns:\n",
    "            # Update existing column with values from the World Bank data\n",
    "            master_df[col] = wb_df.set_index(\"country\")[col].reindex(master_df[\"country\"])\n",
    "        else:\n",
    "            # If the column does not exist, merge it as usual\n",
    "            master_df = pd.merge(\n",
    "                master_df, wb_df[[\"country\", col]], on=\"country\", how=\"left\"\n",
    "            )\n",
    "    return master_df\n",
    "\n",
    "\n",
    "# Extract and clean columns for 2016 and 2017\n",
    "columns_2016 = [col for col in world_bank_data_renamed.columns if col.endswith(\"_2016\")]\n",
    "columns_2017 = [col for col in world_bank_data_renamed.columns if col.endswith(\"_2017\")]\n",
    "\n",
    "world_bank_2016 = strip_year_from_columns(\n",
    "    world_bank_data_renamed[[\"country\"] + columns_2016], 2016\n",
    ")\n",
    "world_bank_2017 = strip_year_from_columns(\n",
    "    world_bank_data_renamed[[\"country\"] + columns_2017], 2017\n",
    ")\n",
    "\n",
    "\n",
    "# Merging 2016 World Bank data with the 2017 master dataset\n",
    "master_dataset_2017_with_wb = merge_world_bank_data(\n",
    "    master_dataset_2017, world_bank_2016, world_bank_2016.columns[1:]\n",
    ")\n",
    "\n",
    "# Merging 2017 World Bank data with the 2018 master dataset\n",
    "master_dataset_2018_with_wb = merge_world_bank_data(\n",
    "    master_dataset_2018, world_bank_2017, world_bank_2017.columns[1:]\n",
    ")\n",
    "\n",
    "# Step 5: Save the merged datasets\n",
    "master_dataset_2017_with_wb.to_csv(\n",
    "    \"./processed_data/master_dataset_2017.csv\", index=False\n",
    ")\n",
    "master_dataset_2018_with_wb.to_csv(\n",
    "    \"./processed_data/master_dataset_2018.csv\", index=False\n",
    ")\n",
    "\n",
    "\n",
    "# Step 6: Checking for missing data in LDCSs\n",
    "def check_missing_data(ldcs, master_df, year):\n",
    "    ldcs_df = master_df[master_df[\"country\"].isin(ldcs[\"country\"])]\n",
    "\n",
    "    # Check for missing values and unstack to get missing per country/variable/year\n",
    "    missing_data = ldcs_df.set_index(\"country\").isnull()\n",
    "\n",
    "    # Melt the DataFrame to create a record of missing data with country and year\n",
    "    missing_records = missing_data.reset_index().melt(\n",
    "        id_vars=[\"country\"], var_name=\"variable\", value_name=\"is_missing\"\n",
    "    )\n",
    "\n",
    "    # Fitler to include only missing entries\n",
    "    missing_records = missing_records[missing_records[\"is_missing\"] == True]\n",
    "\n",
    "    # Add the year directly\n",
    "    missing_records[\"year\"] = year\n",
    "\n",
    "    # Sort the data by year and country\n",
    "    missing_records_sorted = missing_records.sort_values(by=[\"year\", \"country\"])\n",
    "\n",
    "    # Return the relevant columns (country, year, variable) without 'is_missing'\n",
    "    return missing_records_sorted[[\"country\", \"year\", \"variable\"]]\n",
    "\n",
    "\n",
    "# Check missing data for 2017 and 2018 datasets\n",
    "missing_data_2017 = check_missing_data(ldcs_list, master_dataset_2017_with_wb, 2016)\n",
    "missing_data_2018 = check_missing_data(ldcs_list, master_dataset_2018_with_wb, 2017)\n",
    "\n",
    "# Output missing data for review\n",
    "print(\"Missing data for 2017:\")\n",
    "print(missing_data_2017)\n",
    "\n",
    "print(\"Missing data for 2018:\")\n",
    "print(missing_data_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined 2017 and 2018 master spreadsheets into panel dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panel dataset created successfully and saved to: ./processed_data/panel_dataset.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# Define input and output paths\n",
    "input_dir = \"./processed_data\"\n",
    "output_file = \"./processed_data/panel_dataset.csv\"\n",
    "\n",
    "# List of the input files\n",
    "input_files = [\"master_dataset_2017.csv\", \"master_dataset_2018.csv\"]\n",
    "\n",
    "# Read and combine the datasets\n",
    "combined_df = pd.concat(\n",
    "    [pd.read_csv(os.path.join(input_dir, file)) for file in input_files],\n",
    "    ignore_index = True\n",
    ")\n",
    "\n",
    "# Ensuring sorting by \"country\" and \"year\"\n",
    "combined_df = combined_df.sort_values(by=[\"country\", \"year\"]).reset_index(drop=True)\n",
    "\n",
    "# Define the desired column order\n",
    "column_order = [\n",
    "    \"country\",\n",
    "    \"year\",\n",
    "    \"prop_emne\",\n",
    "    \"prop_emne_excl_nat_res\",\n",
    "    \"prop_emne_excl_col_link\",\n",
    "    \"prop_emne_excl_qst_source\",\n",
    "    \"voice_and_acc\",\n",
    "    \"pol_stability\",\n",
    "    \"govt_effectiveness\",\n",
    "    \"reg_quality\",\n",
    "    \"rule_of_law\",\n",
    "    \"control_of_corruption\",\n",
    "    \"gni_per_capita\",\n",
    "    \"broadband_per_capita\",\n",
    "    \"mobiles_per_capita\",\n",
    "    \"phones_per_capita\",\n",
    "    \"geographic_proximity\",\n",
    "    \"colonial_link\",\n",
    "]\n",
    "\n",
    "# Reorder columns if all exist in the dataframe\n",
    "missing_columns = [col for col in column_order if col not in combined_df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Warnig: The following columns are missing from the dataset and will be skipped: {missing_columns}\")\n",
    "column_order = [col for col in column_order if col in combined_df.columns]\n",
    "\n",
    "combined_df = combined_df[column_order]\n",
    "\n",
    "# Save the combined dataset\n",
    "combined_df.to_csv(output_file)\n",
    "\n",
    "print(f\"Panel dataset created successfully and saved to: {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Panel dataset created succesfully. Ready to run statistical tests in Stata."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
