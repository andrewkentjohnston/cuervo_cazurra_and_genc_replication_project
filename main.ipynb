{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import LDCs greenfield FDI info for years 2017-2018. List of LDCs taken from UNCTAD (2017, 2018). Greenfield obtained from FT fDi Markets database and captures all transactions over USD 500,000. \"Source country\" variable is the country of the ultimate parent, confirmed via email with Juanjo Lopez on 29 October, 2024. 2017_2018_greenfield_data_fdi_markets_original.csv saved as \"main_dataset_greenfield.csv\" and all variables and all country names converted to snake_case to faciliate data wrangling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to ./processed_data/greenfield_main_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from fDi Markets\n",
    "fdi_data = \"./raw_data/2017_2018_greenfield_data_fdi_markets_original.csv\"\n",
    "df = pd.read_csv(fdi_data)\n",
    "\n",
    "\n",
    "# Define function to convert text to snake_case\n",
    "def to_snake_case(s):\n",
    "    return s.strip().replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n",
    "\n",
    "\n",
    "# Convert variable names (i.e., column headers) to snake_case for data wrangling\n",
    "df.columns = [to_snake_case(col) for col in df.columns]\n",
    "\n",
    "# Convert values in 'destination_country\" and \"source_country\" to snake_case\n",
    "df[\"destination_country\"] = df[\"destination_country\"].apply(to_snake_case)\n",
    "df[\"source_country\"] = df[\"source_country\"].apply(to_snake_case)\n",
    "\n",
    "# Save modified DataFrame to new CSV file\n",
    "output_filepath = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "df.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"File saved to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create spreadsheet of unique destination countries for data auditing/validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to ./audit_data/unique_destinationa_countries.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afghanistan</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angola</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bangladesh</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>benin</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bhutan</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>burkina_faso</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>burundi</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cambodia</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>central_african_republic</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chad</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>democratic_republic_of_congo</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>djibouti</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>east_timor</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ethiopia</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gambia</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>guinea</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>haiti</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>laos</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lesotho</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>liberia</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>madagascar</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>malawi</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mali</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>mauritania</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mozambique</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>myanmar</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>nepal</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>niger</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rwanda</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>senegal</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>sierra_leone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>somalia</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>sudan</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>tanzania</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>togo</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>uganda</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>zambia</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         country  count\n",
       "0                    afghanistan      3\n",
       "1                         angola     11\n",
       "2                     bangladesh     47\n",
       "3                          benin      8\n",
       "4                         bhutan      6\n",
       "5                   burkina_faso      8\n",
       "6                        burundi      4\n",
       "7                       cambodia     92\n",
       "8       central_african_republic      2\n",
       "9                           chad      2\n",
       "10  democratic_republic_of_congo     15\n",
       "11                      djibouti      3\n",
       "12                    east_timor      2\n",
       "13                      ethiopia     54\n",
       "14                        gambia      4\n",
       "15                        guinea     13\n",
       "16                         haiti      3\n",
       "17                          laos     25\n",
       "18                       lesotho      5\n",
       "19                       liberia      8\n",
       "20                    madagascar      5\n",
       "21                        malawi      7\n",
       "22                          mali      5\n",
       "23                    mauritania      2\n",
       "24                    mozambique     45\n",
       "25                       myanmar    117\n",
       "26                         nepal     15\n",
       "27                         niger      2\n",
       "28                        rwanda     17\n",
       "29                       senegal     20\n",
       "30                  sierra_leone      1\n",
       "31                       somalia      3\n",
       "32                         sudan      3\n",
       "33                      tanzania     42\n",
       "34                          togo      3\n",
       "35                        uganda     34\n",
       "36                        zambia     31"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load processed greenfield dataset\n",
    "filepath = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# Get unique country names and count of each\n",
    "unique_destination_countries = df[\"destination_country\"].value_counts().reset_index()\n",
    "\n",
    "# Rename columns\n",
    "unique_destination_countries.columns = [\"country\", \"count\"]\n",
    "\n",
    "# Sort new df alphabetically\n",
    "unique_destination_countries = unique_destination_countries.sort_values(\n",
    "    by=\"country\"\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_filepath = \"./audit_data/unique_destinationa_countries.csv\"\n",
    "unique_destination_countries.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"File saved to {output_filepath}\")\n",
    "\n",
    "# Display results\n",
    "unique_destination_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare list of destination countries with UNCTAD 2017-2018 LDCs list for data audit/validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDCs not appearing as desintation countries in greenfield_main_dataset.csv:\n",
      "comoros\n",
      "eritrea\n",
      "guinea_bissau\n",
      "kiribati\n",
      "sao_tome_and_principe\n",
      "solomon_islands\n",
      "south_sudan\n",
      "tuvalu\n",
      "vanuatu\n",
      "yemen\n",
      "Countries appearing as destination countires in greenfield_main_dataset.csv but not appearing in LDCs list:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both spreadsheets into DataFrames\n",
    "destination_countries_df = pd.read_csv(\"./audit_data/unique_destinationa_countries.csv\")\n",
    "ldcs_df = pd.read_csv(\"./variable_data/ldcs_list_2017_2018.csv\")\n",
    "\n",
    "\n",
    "# Convert country names to snake_case\n",
    "def to_snake_case(s):\n",
    "    return s.strip().replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n",
    "\n",
    "\n",
    "destination_countries_df[\"country\"] = destination_countries_df[\"country\"].apply(\n",
    "    to_snake_case\n",
    ")\n",
    "ldcs_df[\"country\"] = ldcs_df[\"country\"].apply(to_snake_case)\n",
    "\n",
    "# Convert pandas Series objects into sets for comparison\n",
    "destination_countries_set = set(destination_countries_df[\"country\"])\n",
    "ldcs_set = set(ldcs_df[\"country\"])\n",
    "\n",
    "# Create new sets of missing countries\n",
    "missing_in_destination_countries_list = sorted(ldcs_set - destination_countries_set)\n",
    "missing_in_ldcs_list = sorted(destination_countries_set - ldcs_set)\n",
    "\n",
    "# Display results\n",
    "print(\"LDCs not appearing as desintation countries in greenfield_main_dataset.csv:\")\n",
    "for country in missing_in_destination_countries_list:\n",
    "    print(country)\n",
    "print(\n",
    "    \"Countries appearing as destination countries in greenfield_main_dataset.csv but not appearing in LDCs list:\"\n",
    ")\n",
    "for country in missing_in_ldcs_list:\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDCs not appearing in greenfield FDI are consistent with those for which there was no data in fDi Markets for those years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create spreadsheet of unique source countries for data auditing/validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to ./audit_data/unique_source_countries.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>australia</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>austria</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>azerbaijan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>barbados</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>belgium</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>uae</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ukraine</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>united_kingdom</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>united_states</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>vietnam</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           country  count\n",
       "0        australia      4\n",
       "1          austria      2\n",
       "2       azerbaijan      1\n",
       "3         barbados      1\n",
       "4          belgium      6\n",
       "..             ...    ...\n",
       "58             uae     26\n",
       "59         ukraine      4\n",
       "60  united_kingdom     28\n",
       "61   united_states     49\n",
       "62         vietnam     23\n",
       "\n",
       "[63 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load processed greenfield dataset\n",
    "filepath = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# Get unique source country names and count of each\n",
    "unique_source_countries = df[\"source_country\"].value_counts().reset_index()\n",
    "\n",
    "# Rename columns\n",
    "unique_source_countries.columns = [\"country\", \"count\"]\n",
    "\n",
    "# Sort new df alphabetically\n",
    "unique_source_countries = unique_source_countries.sort_values(by=\"country\").reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "output_filepath = \"./audit_data/unique_source_countries.csv\"\n",
    "unique_source_countries.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"File saved to {output_filepath}\")\n",
    "\n",
    "# Display results\n",
    "unique_source_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare list of source countries with UNCTAD 2017-2018 developed country list for data audit/validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developed countries not appearing as source countries in greenfield_main_dataset.csv:\n",
      "andorra\n",
      "bermuda\n",
      "bulgaria\n",
      "czechia\n",
      "estonia\n",
      "faroe_islands\n",
      "gibraltar\n",
      "greece\n",
      "greenland\n",
      "holy_see\n",
      "hungary\n",
      "iceland\n",
      "ireland\n",
      "latvia\n",
      "lithuania\n",
      "luxembourg\n",
      "malta\n",
      "poland\n",
      "romania\n",
      "saint_pierre_and_miquelon\n",
      "san_marino\n",
      "slovakia\n",
      "slovenia\n",
      "sweden\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both spreadsheets into DataFrames\n",
    "source_countries_df = pd.read_csv(\"./audit_data/unique_source_countries.csv\")\n",
    "developed_countries_df = pd.read_csv(\n",
    "    \"./variable_data/developed_countries_2017_2018.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "# Convert country names to snake_case\n",
    "def to_snake_case(s):\n",
    "    return s.strip().replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n",
    "\n",
    "\n",
    "source_countries_df[\"country\"] = source_countries_df[\"country\"].apply(to_snake_case)\n",
    "developed_countries_df[\"country\"] = developed_countries_df[\"country\"].apply(\n",
    "    to_snake_case\n",
    ")\n",
    "\n",
    "# Convert pandas Series objects into sets for comparison\n",
    "source_countries_set = set(source_countries_df[\"country\"])\n",
    "developed_countries_set = set(developed_countries_df[\"country\"])\n",
    "\n",
    "# Create new set of missing countries\n",
    "missing_in_source_countries = developed_countries_set - source_countries_set\n",
    "\n",
    "# Display results\n",
    "print(\n",
    "    \"Developed countries not appearing as source countries in greenfield_main_dataset.csv:\"\n",
    ")\n",
    "for country in sorted(missing_in_source_countries):\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developed countries not appearing in greenfield_main_dataset were checked manually against unique_source_countries.csv to ensure no matches were missed due to spelling or punctuation issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare list of countries in tax_havens.csv with countries appearing in source_country column and identify any tax_haven countries not appearing as source countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tax haven countries not appearing as source countries in greenfield_main_dataset.csv:\n",
      "andorra\n",
      "anguilla\n",
      "antigua_and_barbuda\n",
      "aruba\n",
      "bahamas\n",
      "bahrain\n",
      "bermuda\n",
      "british_virgin_islands\n",
      "cayman_islands\n",
      "gibraltar\n",
      "grenada\n",
      "guernsey\n",
      "ireland\n",
      "isle_of_man\n",
      "jersey\n",
      "lebanon\n",
      "liechtenstein\n",
      "luxembourg\n",
      "macau\n",
      "malta\n",
      "marshall_islands\n",
      "monaco\n",
      "netherlands_antilles\n",
      "panama\n",
      "puerto_rico\n",
      "samoa\n",
      "seychelles\n",
      "st_kitts_and_nevis\n",
      "st_lucia\n",
      "st_vincent_and_grenadines\n",
      "turks_and_caicos\n",
      "vanuatu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both datasets\n",
    "source_countries_df = pd.read_csv(\"./audit_data/unique_source_countries.csv\")\n",
    "tax_havens_df = pd.read_csv(\"./variable_data/tax_havens.csv\")\n",
    "\n",
    "\n",
    "# Covert country names to snake_case\n",
    "def to_snake_case(s):\n",
    "    return s.strip().replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n",
    "\n",
    "\n",
    "source_countries_df[\"countries\"] = source_countries_df[\"country\"].apply(to_snake_case)\n",
    "tax_havens_df[\"country\"] = tax_havens_df[\"country\"].apply(to_snake_case)\n",
    "\n",
    "# Convert Pandas Series objects into sets for comparison\n",
    "source_countries_set = set(source_countries_df[\"country\"])\n",
    "tax_havens_set = set(tax_havens_df[\"country\"])\n",
    "\n",
    "# Create set of missing countries\n",
    "missing_tax_havens_set = tax_havens_set - source_countries_set\n",
    "\n",
    "# Display results\n",
    "print(\n",
    "    \"Tax haven countries not appearing as source countries in greenfield_main_dataset.csv:\"\n",
    ")\n",
    "\n",
    "for country in sorted(missing_tax_havens_set):\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tax haven countries not appearing in greenfield_main_dataset were checked manually against unique_source_countries.csv to ensure no matches were missed due to spelling or punctuation issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create source_is_th variable. Value will be True if source_country appears in list of tax_havens and False otherwise. Tax havens list taken from footnote 10 on page 1506 of Tørsløv, T., Wier, L., & Zucman, G. (2023). The missing profits of nations. The Review of Economic Studies, 90(3), 1499-1534."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greenfield_main_dataset.csv updated with \"source_is_th\" variable.\n",
      "Number of observations where \"source_is_th\" is True: 96\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the spreadsheets into DataFrames\n",
    "\n",
    "greenfield_df = pd.read_csv(\"./processed_data/greenfield_main_dataset.csv\")\n",
    "tax_havens_df = pd.read_csv(\"./variable_data/tax_havens.csv\")\n",
    "\n",
    "# Create set of tax havens\n",
    "tax_havens_set = set(tax_havens_df[\"country\"])\n",
    "\n",
    "# Add \"source_is_th\" variable to greenfield_df\n",
    "greenfield_df[\"source_is_th\"] = greenfield_df[\"source_country\"].isin(tax_havens_set)\n",
    "\n",
    "greenfield_df.to_csv(\"./processed_data/greenfield_main_dataset.csv\", index=False)\n",
    "\n",
    "print('greenfield_main_dataset.csv updated with \"source_is_th\" variable.')\n",
    "\n",
    "# Output the number of observations where \"destination_is_th\" is True to console\n",
    "true_count = greenfield_df[\"source_is_th\"].sum()\n",
    "print(f'Number of observations where \"source_is_th\" is True: {true_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create destination_is_th variable. Value will be True if destination_country appears in list of tax_havens and False otherwise. Tax havens list taken from footnote 10 on page 1506 of Tørsløv, T., Wier, L., & Zucman, G. (2023). The missing profits of nations. The Review of Economic Studies, 90(3), 1499-1534."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greenfield_main_dataset.csv updated with \"destination_is_th\" variable.\n",
      "Number of observations where \"destination_is_th\" is True: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the spreadsheets into DataFrames\n",
    "\n",
    "greenfield_df = pd.read_csv(\"./processed_data/greenfield_main_dataset.csv\")\n",
    "tax_havens_df = pd.read_csv(\"./variable_data/tax_havens.csv\")\n",
    "\n",
    "# Create set of tax havens\n",
    "tax_havens_set = set(tax_havens_df[\"country\"])\n",
    "\n",
    "# Add \"destination_is_th\" variable to greenfield_df\n",
    "greenfield_df[\"destination_is_th\"] = greenfield_df[\"destination_country\"].isin(tax_havens_set)\n",
    "\n",
    "greenfield_df.to_csv(\"./processed_data/greenfield_main_dataset.csv\", index=False)\n",
    "\n",
    "print('greenfield_main_dataset.csv updated with \"destination_is_th\" variable.')\n",
    "\n",
    "# Output the number of observations where \"destination_is_th\" is True to console\n",
    "true_count = greenfield_df[\"destination_is_th\"].sum()\n",
    "print(f'Number of observations where \"destination_is_th\" is True: {true_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset comprising all observations where source_country is a tax haven and manually check accuracy as true origin of FDI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to greenfield_source_is_th_unique.csv\n",
      "Number of unique parent companies in filtered data: 63\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "greenfield_df = pd.read_csv(\"./processed_data/greenfield_main_dataset.csv\")\n",
    "\n",
    "# Filter for those instances were destination_is_th == True\n",
    "greenfield_source_is_th_df = greenfield_df[greenfield_df[\"source_is_th\"] == True]\n",
    "\n",
    "# Select specific columns\n",
    "selected_columns = [\n",
    "    \"parent_company\",\n",
    "    \"company_profile\",\n",
    "    \"website\",\n",
    "    \"source_country\",\n",
    "    \"sector\",\n",
    "    \"sub_sector\",\n",
    "    \"cluster\",\n",
    "    \"activity\",\n",
    "]\n",
    "\n",
    "# Retain only unique rows based on 'parent_company' and selected columns\n",
    "unique_parent_company_df = greenfield_source_is_th_df[\n",
    "    selected_columns\n",
    "].drop_duplicates(subset=[\"parent_company\"])\n",
    "\n",
    "# Convert new dataframe to CSV for manual checking\n",
    "unique_parent_company_df.to_csv(\n",
    "    \"./audit_data/greenfield_source_is_th_unique.csv\"\n",
    ")\n",
    "\n",
    "num_observations = unique_parent_company_df.shape[0]\n",
    "\n",
    "print(\"Filtered data saved to greenfield_source_is_th_unique.csv\")\n",
    "print(f'Number of unique parent companies in filtered data: {num_observations}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually check all companies to determine accuracy of parent country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"developed\" variable in greenfield_main_dataset.csv. Variable takes value of True if source country appears in list of developed countries and False otherwise. Also created \"emne\" variable that takes the inverse value of the \"developed\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greenfield_main_dataset.csv updated with \"developed\" and \"emne\" variables.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load spreadsheets into DataFrames\n",
    "\n",
    "greenfield_filepath = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "developed_countries_filepath = \"./variable_data/developed_countries_2017_2018.csv\"\n",
    "\n",
    "greenfield_df = pd.read_csv(greenfield_filepath)\n",
    "developed_countries_df = pd.read_csv(developed_countries_filepath)\n",
    "\n",
    "# Create set of developed countries\n",
    "developed_countries_set = set(developed_countries_df[\"country\"])\n",
    "\n",
    "# Add \"developed\" variable to greenfield_df\n",
    "greenfield_df[\"developed\"] = greenfield_df[\"source_country\"].isin(\n",
    "    developed_countries_set\n",
    ")\n",
    "\n",
    "# Add \"enne\" variable to greenfield_df\n",
    "greenfield_df[\"emne\"] = ~greenfield_df[\"source_country\"].isin(developed_countries_set)\n",
    "\n",
    "# Save the updated DataFrame as greenfield_main_dataset.csv\n",
    "greenfield_df.to_csv(greenfield_filepath, index=False)\n",
    "\n",
    "print(f'greenfield_main_dataset.csv updated with \"developed\" and \"emne\" variables.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"bordering_country\" variable. Variable is True if source countries shares a border with destination country and False otherwise. Bordering countries checked from CIA World Factbook https://www.cia.gov/the-world-factbook/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new variable \"bordering country\" has been added and the updated file has been saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load spreadsheets into DataFrames\n",
    "greenfield_filepath = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "bordering_countries_filepath = \"./variable_data/bordering_countries_2017_2018.csv\"\n",
    "\n",
    "greenfield_df = pd.read_csv(greenfield_filepath)\n",
    "bordering_countries_df = pd.read_csv(bordering_countries_filepath)\n",
    "\n",
    "# Create dictionary of bordering countries\n",
    "bordering_countries_dict = bordering_countries_df.set_index(\"ldc\")[\n",
    "    \"bordering_countries\"\n",
    "].to_dict()\n",
    "\n",
    "\n",
    "# Function to check if the source country is in the list of bordering countries\n",
    "def is_bordering(source_country, destination_country):\n",
    "    bordering_countries = bordering_countries_dict.get(destination_country, \"\")\n",
    "    if pd.isna(bordering_countries) or not bordering_countries:\n",
    "        return False\n",
    "    bordering_list = bordering_countries.split(\", \")\n",
    "    return source_country in bordering_list\n",
    "\n",
    "\n",
    "# Apply the is_bordering function to create the new \"bordering_country\" variable in greenfield_df\n",
    "greenfield_df[\"bordering_country\"] = greenfield_df.apply(\n",
    "    lambda row: is_bordering(row[\"source_country\"], row[\"destination_country\"]), axis=1\n",
    ")\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "greenfield_df.to_csv(greenfield_filepath, index=False)\n",
    "\n",
    "print(\n",
    "    f'The new variable \"bordering country\" has been added and the updated file has been saved.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new \"industry\" variable by joining \"sector\", \"sub_sector\", and \"activity\". Create list/spreadsheet of all unique values in \"industry\" column (and their count) from greenfield_main_dataset.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset has been saved to ./processed_data/greenfield_main_dataset.csv.\n",
      "        Industry counts has been saved to ./audit_data/industry_counts.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load processed greenfield dataset\n",
    "filepath = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "\n",
    "# Define function to convert strings to snake_case\n",
    "def to_snake_case(s):\n",
    "    s = (\n",
    "        s.replace(\" \", \"_\")\n",
    "        .replace(\"&\", \"and\")\n",
    "        .replace(\",\", \"\")\n",
    "        .replace(\"/\", \"\")\n",
    "        .replace(\"-\", \"_\")\n",
    "        .replace(\",\", \"\")\n",
    "        .replace(\"(\", \"\")\n",
    "        .replace(\")\", \"\")\n",
    "    )\n",
    "    return s.lower()\n",
    "\n",
    "\n",
    "# Create a new 'industry' column by joining 'sector', 'sub-sector', and 'activity'\n",
    "df[\"industry\"] = (\n",
    "    df[[\"sector\", \"sub_sector\", \"activity\"]]\n",
    "    .fillna(\"\")\n",
    "    .apply(lambda x: \" \".join(x), axis=1)\n",
    ")\n",
    "\n",
    "# Convert the new 'industry' column to snake_case\n",
    "df[\"industry\"] = df[\"industry\"].apply(to_snake_case)\n",
    "\n",
    "# Save the modified DataFrame to CSV\n",
    "df.to_csv(filepath, index=False)\n",
    "\n",
    "# Calculate the count of unique values in 'industry' column\n",
    "industry_counts = df[\"industry\"].value_counts().reset_index()\n",
    "industry_counts.columns = [\"industry\", \"count\"]\n",
    "\n",
    "# Sort alphabetically\n",
    "industry_counts.sort_values(by=\"industry\", inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "industry_counts_filepath = \"./audit_data/industry_counts.csv\"\n",
    "industry_counts.to_csv(industry_counts_filepath, index=False)\n",
    "\n",
    "print(\n",
    "    f\"Updated dataset has been saved to {filepath}.\\n\\\n",
    "        Industry counts has been saved to {industry_counts_filepath}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"natural_resource\" variable. Variable is True if industry is listed as a natural resource industry in the \"nat_resource_list.csv\" spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The modified dataset has been saved to ./processed_data/greenfield_main_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both spreadsheets into DataFrames\n",
    "main_dataset_path = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "nat_resource_list_path = \"./variable_data/nat_resource_list.csv\"\n",
    "\n",
    "main_df = pd.read_csv(main_dataset_path)\n",
    "nat_resource_df = pd.read_csv(nat_resource_list_path)\n",
    "\n",
    "# Create a set of natural resource industries\n",
    "natural_resource_set = set(\n",
    "    nat_resource_df[nat_resource_df[\"natural_resource\"] == True][\"industry\"]\n",
    ")\n",
    "\n",
    "# Create a new column \"natural_resource_ind\" in main_df\n",
    "main_df[\"natural_resource_ind\"] = main_df[\"industry\"].apply(\n",
    "    lambda x: x in natural_resource_set\n",
    ")\n",
    "\n",
    "# Save the modified DataFrame to CSV\n",
    "main_df.to_csv(main_dataset_path, index=False)\n",
    "\n",
    "print(f\"The modified dataset has been saved to {main_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"colonial_link\" variable. Variable is True if the source country is listed in the \"colonial_rulers\" column for that country in \"colonial_rulers.csv\" and False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset saved to ./processed_data/greenfield_main_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both spreadsheets into DataFrames\n",
    "main_dataset_path = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "colonial_rulers_path = \"./variable_data/colonial_rulers_list.csv\"\n",
    "\n",
    "main_df = pd.read_csv(main_dataset_path)\n",
    "colonial_rulers_df = pd.read_csv(colonial_rulers_path)\n",
    "\n",
    "# Create a dictionary from colonial_rulers_list\n",
    "colonial_rulers_dict = colonial_rulers_df.set_index(\"country\")[\n",
    "    \"colonial_rulers\"\n",
    "].to_dict()\n",
    "\n",
    "\n",
    "# Function to check for colonial link\n",
    "def has_colonial_link(row):\n",
    "    destination = row[\"destination_country\"]\n",
    "    source = row[\"source_country\"]\n",
    "    colonial_rulers = colonial_rulers_dict.get(destination)\n",
    "\n",
    "    if isinstance(colonial_rulers, str):\n",
    "        colonial_rulers_list = [\n",
    "            ruler.strip().replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n",
    "            for ruler in colonial_rulers.split(\", \")\n",
    "        ]\n",
    "        return source in colonial_rulers_list\n",
    "    return False\n",
    "\n",
    "\n",
    "# Apply the function to create new \"colonial_link\" column\n",
    "main_df[\"colonial_link\"] = main_df.apply(has_colonial_link, axis=1)\n",
    "\n",
    "# Save the updated DatafRame to CSV\n",
    "main_df.to_csv(main_dataset_path, index=False)\n",
    "\n",
    "print(f\"Updated dataset saved to {main_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 2017 and 2018 spreadsheets for each individual LDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country spreadsheets created and saved to \"country_spreadsheets/greenfield\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load both spreadsheets into DataFrames\n",
    "main_dataset_path = \"./processed_data/greenfield_main_dataset.csv\"\n",
    "ldcs_list_path = \"./variable_data/ldcs_list_2017_2018.csv\"\n",
    "\n",
    "main_df = pd.read_csv(main_dataset_path)\n",
    "ldcs_df = pd.read_csv(ldcs_list_path)\n",
    "\n",
    "\n",
    "# Define function to convert country names to snake_case\n",
    "def to_snake_case(s):\n",
    "    return s.strip().replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n",
    "\n",
    "\n",
    "# Convert country names in both DataFrames to snake_case\n",
    "main_df[\"destination_country\"].apply(to_snake_case)\n",
    "ldcs_df[\"country\"].apply(to_snake_case)\n",
    "\n",
    "# Convert \"project_date\" to datetime for easier filtering\n",
    "main_df[\"project_date\"] = pd.to_datetime(main_df[\"project_date\"], format=\"%b %Y\")\n",
    "\n",
    "# Extract year from \"project_date\"\n",
    "main_df[\"year\"] = main_df[\"project_date\"].dt.year\n",
    "\n",
    "# Create output directories for country spreadsheets\n",
    "output_dir = \"./country_spreadsheets/greenfield\"\n",
    "\n",
    "for year in [2017, 2018]:\n",
    "    year_dir = os.path.join(output_dir, str(year))\n",
    "    if not os.path.exists(year_dir):\n",
    "        os.makedirs(year_dir)\n",
    "\n",
    "# Create list to keep track of rows not assigned to a spreadhseet\n",
    "unassigned_rows = []\n",
    "\n",
    "# List to keep track of empty spreadsheets\n",
    "empty_spreadsheets = []\n",
    "\n",
    "# Proess each country in the ldcs_df\n",
    "for country in ldcs_df[\"country\"]:\n",
    "    for year in [2017, 2018]:\n",
    "        # Filter data for the specific country and year\n",
    "        country_year_data = main_df[\n",
    "            (main_df[\"destination_country\"] == country) & (main_df[\"year\"] == year)\n",
    "        ]\n",
    "\n",
    "        # Create filename\n",
    "        year_dir = os.path.join(output_dir, str(year))\n",
    "        filename = f\"{year_dir}/{country}_greenfield_{year}.csv\"\n",
    "\n",
    "        # Save data to CSV\n",
    "        if not country_year_data.empty:\n",
    "            country_year_data.to_csv(filename, index=False)\n",
    "        else:\n",
    "            # Save an empty CSV with the same headers as greenfield_main_dataset.csv\n",
    "            empty_df = main_df.head(0)\n",
    "            empty_df.to_csv(filename, index=False)\n",
    "            empty_spreadsheets.append(filename)\n",
    "\n",
    "# Identify rows not assigned to any country spreadsheet\n",
    "assigned_rows = main_df[main_df[\"destination_country\"].isin(ldcs_df[\"country\"])]\n",
    "unassigned_rows = main_df[~main_df.index.isin(assigned_rows.index)]\n",
    "\n",
    "# Save unassigned rows to a CSV file\n",
    "audit_dir = \"./audit_data\"\n",
    "unassigned_filename = f\"{audit_dir}/unassigned_rows.csv\"\n",
    "unassigned_rows.to_csv(unassigned_filename, index=False)\n",
    "\n",
    "# Save list of empty spreadsheets to a CSV file\n",
    "empty_spreadsheets_filename = f\"{audit_dir}/empty_spreadsheets.csv\"\n",
    "pd.DataFrame(empty_spreadsheets, columns=[\"empty_spreadsheets\"]).to_csv(\n",
    "    empty_spreadsheets_filename, index=False\n",
    ")\n",
    "\n",
    "print('Country spreadsheets created and saved to \"country_spreadsheets/greenfield\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create master spreadsheets for 2017 and 2018 that show greenfield FDI total; total excluding natural resource industries, and total excluding firms from former colonial powers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master spreadsheets created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory paths\n",
    "base_dir = \"./country_spreadsheets\"\n",
    "years = [\"2017\", \"2018\"]\n",
    "output_dir = \"./processed_data\"\n",
    "\n",
    "# Initialize an empty list for each year to hold data\n",
    "data_2017 = []\n",
    "data_2018 = []\n",
    "\n",
    "\n",
    "# Function to process each file and extract the required information\n",
    "def process_file(filepath, country, year):\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    total_greenfield = df[\"capital_investment\"].sum()\n",
    "    total_greenfield_excl_nat_res = df[df[\"natural_resource_ind\"] == False][\n",
    "        \"capital_investment\"\n",
    "    ].sum()\n",
    "    total_greenfield_excl_col_link = df[df[\"colonial_link\"] == False][\n",
    "        \"capital_investment\"\n",
    "    ].sum()\n",
    "    emne_greenfield = df[df[\"emne\"] == True][\"capital_investment\"].sum()\n",
    "    emne_greenfield_exl_nat_res = df[\n",
    "        (df[\"emne\"] == True) & (df[\"natural_resource_ind\"] == False)\n",
    "    ][\"capital_investment\"].sum()\n",
    "    emne_greenfield_exl_col_link = df[\n",
    "        (df[\"emne\"] == True) & (df[\"colonial_link\"] == False)\n",
    "    ][\"capital_investment\"].sum()\n",
    "\n",
    "    return {\n",
    "        \"country\": country,\n",
    "        \"year\": year,\n",
    "        \"total_greenfield\": total_greenfield,\n",
    "        \"total_greenefield_excl_nat_res\": total_greenfield_excl_nat_res,\n",
    "        \"total_greenfield_excl_col_link\": total_greenfield_excl_col_link,\n",
    "        \"emne_greenfield\": emne_greenfield,\n",
    "        \"emne_greenfield_excl_nat_res\": emne_greenfield_exl_nat_res,\n",
    "        \"emne_greenfield_excl_col_link\": emne_greenfield_exl_col_link,\n",
    "    }\n",
    "\n",
    "\n",
    "# Loop through each year and process the files\n",
    "for year in years:\n",
    "    year_dir = os.path.join(base_dir, \"greenfield\", year)\n",
    "    for file_name in os.listdir(year_dir):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            parts = file_name.split(\"_\")\n",
    "\n",
    "            # Extract country name by joining all parts except the last two\n",
    "            country = \"_\".join(parts[:-2])\n",
    "            filepath = os.path.join(year_dir, file_name)\n",
    "            data = process_file(filepath, country, year)\n",
    "            if year == \"2017\":\n",
    "                data_2017.append(data)\n",
    "            else:\n",
    "                data_2018.append(data)\n",
    "\n",
    "# Create DataFrames for 2017 and 2018\n",
    "df_2017 = pd.DataFrame(data_2017)\n",
    "df_2018 = pd.DataFrame(data_2018)\n",
    "\n",
    "# Sort the DataFrames\n",
    "df_2017 = df_2017.sort_values(by=[\"country\"])\n",
    "df_2018 = df_2018.sort_values(by=[\"country\"])\n",
    "\n",
    "# Save the DataFrames to CSV\n",
    "df_2017.to_csv(os.path.join(output_dir, \"master_dataset_2017.csv\"), index=False)\n",
    "df_2018.to_csv(os.path.join(output_dir, \"master_dataset_2018.csv\"), index=False)\n",
    "\n",
    "print(\"Master spreadsheets created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add governance variables and control variables, all of which can be downloaded for LDCs from World Bank DataBank here:\n",
    "https://databank.worldbank.org/source/world-development-indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data for 2017:\n",
      "                   country  year                   variable\n",
      "423            afghanistan  2016        gni_per_capita_2016\n",
      "425             bangladesh  2016        gni_per_capita_2016\n",
      "437                eritrea  2016        gni_per_capita_2016\n",
      "443               kiribati  2016        gni_per_capita_2016\n",
      "446                liberia  2016        gni_per_capita_2016\n",
      "448                 malawi  2016        gni_per_capita_2016\n",
      "452                myanmar  2016        gni_per_capita_2016\n",
      "456  sao_tome_and_principe  2016        gni_per_capita_2016\n",
      "364           sierra_leone  2016  broadband_per_capita_2016\n",
      "460                somalia  2016        gni_per_capita_2016\n",
      "461            south_sudan  2016        gni_per_capita_2016\n",
      "462                  sudan  2016        gni_per_capita_2016\n",
      "465                 tuvalu  2016        gni_per_capita_2016\n",
      "468                  yemen  2016        gni_per_capita_2016\n",
      "469                 zambia  2016        gni_per_capita_2016\n",
      "Missing data for 2018:\n",
      "                   country  year                   variable\n",
      "423            afghanistan  2017        gni_per_capita_2017\n",
      "425             bangladesh  2017        gni_per_capita_2017\n",
      "437                eritrea  2017        gni_per_capita_2017\n",
      "443               kiribati  2017        gni_per_capita_2017\n",
      "444                   laos  2017        gni_per_capita_2017\n",
      "446                liberia  2017        gni_per_capita_2017\n",
      "587                liberia  2017     phones_per_capita_2017\n",
      "448                 malawi  2017        gni_per_capita_2017\n",
      "452                myanmar  2017        gni_per_capita_2017\n",
      "456  sao_tome_and_principe  2017        gni_per_capita_2017\n",
      "364           sierra_leone  2017  broadband_per_capita_2017\n",
      "460                somalia  2017        gni_per_capita_2017\n",
      "601                somalia  2017     phones_per_capita_2017\n",
      "461            south_sudan  2017        gni_per_capita_2017\n",
      "462                  sudan  2017        gni_per_capita_2017\n",
      "465                 tuvalu  2017        gni_per_capita_2017\n",
      "372                 uganda  2017  broadband_per_capita_2017\n",
      "373                vanuatu  2017  broadband_per_capita_2017\n",
      "468                  yemen  2017        gni_per_capita_2017\n",
      "469                 zambia  2017        gni_per_capita_2017\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "world_bank_data = pd.read_csv(\"./raw_data/world_bank_data_2016_2017.csv\")\n",
    "master_dataset_2017 = pd.read_csv(\"./processed_data/master_dataset_2017.csv\")\n",
    "master_dataset_2018 = pd.read_csv(\"./processed_data/master_dataset_2018.csv\")\n",
    "world_bank_var_names = pd.read_csv(\"./audit_data/world_bank_var_names.csv\")\n",
    "name_conversions = pd.read_csv(\"./audit_data/world_bank_name_conversions.csv\")\n",
    "ldcs_list = pd.read_csv(\"./variable_data/ldcs_list_2017_2018.csv\")\n",
    "\n",
    "\n",
    "# Step 1: Standardize country names\n",
    "def standardize_country_names(df, conversion_df):\n",
    "    # Apply name mapping\n",
    "    name_map = conversion_df.set_index(\"old_name\")[\"new_name\"].dropna().to_dict()\n",
    "    df[\"country\"] = df[\"Country Name\"].replace(name_map)\n",
    "\n",
    "    # Then convert the names to snake_case\n",
    "    df[\"country\"] = (\n",
    "        df[\"country\"]\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\", regex=False)\n",
    "        .str.replace(\"-\", \"_\", regex=False)\n",
    "        .str.replace(\",\", \"_\", regex=False)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Replace \"..\" values in world_bank_data with NaN\n",
    "world_bank_data.replace(\"..\", pd.NA, inplace=True)\n",
    "\n",
    "# Apply the name conversion process\n",
    "world_bank_data = standardize_country_names(world_bank_data, name_conversions)\n",
    "\n",
    "# Step 2: Rename variables based on world_bank_var_names.csv and reshape data\n",
    "var_name_map = world_bank_var_names.set_index(\"old_name\")[\"new_name\"].to_dict()\n",
    "\n",
    "\n",
    "def rename_world_bank_variables(df, var_map):\n",
    "    df[\"Series Name\"] = df[\"Series Name\"].replace(var_map)\n",
    "    df = df.pivot(\n",
    "        index=\"country\",\n",
    "        columns=\"Series Name\",\n",
    "        values=[\"2016 [YR2016]\", \"2017 [YR2017]\"],\n",
    "    )\n",
    "    df.columns = [\n",
    "        f\"{col[1]}_{col[0][:4]}\" for col in df.columns.values\n",
    "    ]  # Format as, e.g. \"gni_per_capita_2016\"\n",
    "    return df.reset_index()\n",
    "\n",
    "\n",
    "# Rename variables and reshape data\n",
    "world_bank_data_renamed = rename_world_bank_variables(world_bank_data, var_name_map)\n",
    "\n",
    "\n",
    "# Step 3: Merge transformed World Bank data with the master datasets\n",
    "def merge_world_bank_data(master_df, wb_df, year_columns):\n",
    "    for col in year_columns:\n",
    "        # Check if the column already exists in the master dataset\n",
    "        if col in master_df.columns:\n",
    "            # Update existing column with values from the World Bank data\n",
    "            master_df[col].update(wb_df.set_index(\"country\")[col])\n",
    "        else:\n",
    "            # If the column does not exist, merge it as usual\n",
    "            master_df = pd.merge(\n",
    "                master_df, wb_df[[\"country\", col]], on=\"country\", how=\"left\"\n",
    "            )\n",
    "    return master_df\n",
    "\n",
    "\n",
    "# Extract relevant columns for 2016 and 2017 from world_bank_data_renamed\n",
    "columns_2016 = [col for col in world_bank_data_renamed.columns if col.endswith(\"_2016\")]\n",
    "columns_2017 = [col for col in world_bank_data_renamed.columns if col.endswith(\"_2017\")]\n",
    "\n",
    "# Merging 2016 World Bank data with the 2017 master dataset\n",
    "master_dataset_2017_with_wb = merge_world_bank_data(\n",
    "    master_dataset_2017, world_bank_data_renamed, columns_2016\n",
    ")\n",
    "\n",
    "# Merging 2017 World Bank data with the 2018 master dataset\n",
    "master_dataset_2018_with_wb = merge_world_bank_data(\n",
    "    master_dataset_2018, world_bank_data_renamed, columns_2017\n",
    ")\n",
    "\n",
    "# Save the merged datasets\n",
    "master_dataset_2017_with_wb.to_csv(\n",
    "    \"./processed_data/master_dataset_2017.csv\", index=False\n",
    ")\n",
    "master_dataset_2018_with_wb.to_csv(\n",
    "    \"./processed_data/master_dataset_2018.csv\", index=False\n",
    ")\n",
    "\n",
    "\n",
    "# Checking for missing data in LDCSs\n",
    "def check_missing_data(ldcs, master_df):\n",
    "    ldcs_df = master_df[master_df[\"country\"].isin(ldcs[\"country\"])]\n",
    "\n",
    "    # Check for missing values and unstack to get missing per country/variable/year\n",
    "    missing_data = ldcs_df.set_index(\"country\").isnull()\n",
    "\n",
    "    # Melt the DataFrame to create a record of missing data with country and year\n",
    "    missing_records = missing_data.reset_index().melt(\n",
    "        id_vars=[\"country\"], var_name=\"variable\", value_name=\"is_missing\"\n",
    "    )\n",
    "\n",
    "    # Fitler to include only missing entries\n",
    "    missing_records = missing_records[missing_records[\"is_missing\"] == True]\n",
    "\n",
    "    # Extract the year from the variable name (e.g., \"gni_per_capita_2017\" -> 2017)\n",
    "    missing_records[\"year\"] = missing_records[\"variable\"].str.extract(r\"(\\d{4})\")\n",
    "\n",
    "    # Sort the data by year and country\n",
    "    missing_records_sorted = missing_records.sort_values(by=[\"year\", \"country\"])\n",
    "\n",
    "    # Return the relevant columns (country, year, variable) without 'is_missing'\n",
    "    return missing_records_sorted[[\"country\", \"year\", \"variable\"]]\n",
    "\n",
    "\n",
    "# Check missing data for 2017 and 2018 datasets\n",
    "missing_data_2017 = check_missing_data(ldcs_list, master_dataset_2017_with_wb)\n",
    "missing_data_2018 = check_missing_data(ldcs_list, master_dataset_2018_with_wb)\n",
    "\n",
    "# Output missing data for review\n",
    "print(\"Missing data for 2017:\")\n",
    "print(missing_data_2017)\n",
    "\n",
    "print(\"Missing data for 2018:\")\n",
    "print(missing_data_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tax_haven variable. Value will be True if source_country appears in list of tax_havens and False otherwise. Tax havens list taken from footnote 10 on page 1506 of Tørsløv, T., Wier, L., & Zucman, G. (2023). The missing profits of nations. The Review of Economic Studies, 90(3), 1499-1534."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding M&A data from SDC Platinum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values saved to ./audit_data/m_and_a_unique_destination_countries.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "input_filepath = \"./raw_data/m_and_a_data_2017_2018.csv\"\n",
    "output_filepath = \"./audit_data/m_and_a_unique_destination_countries.csv\"\n",
    "\n",
    "data = pd.read_csv(input_filepath)\n",
    "\n",
    "# Get unique values and their counts from TNATION column\n",
    "unique_values_counts = data[\"TNATION\"].value_counts().reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "unique_values_counts.columns = [\"country\", \"count\"]\n",
    "\n",
    "# Sort df alphabetically\n",
    "unique_values_counts = unique_values_counts.sort_values(by=\"country\")\n",
    "# Save the new dataframe to a CSV file\n",
    "unique_values_counts.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"Unique values saved to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out non-LDC destination countries from M&A data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDC countries not appearing in the M&A dataset's destination_country column:\n",
      "comoros\n",
      "guinea_bissau\n",
      "kiribati\n",
      "sao_tome_and_principe\n",
      "south_sudan\n",
      "tuvalu\n",
      "Filtered data saved to ./processed_data/m_and_a_main_dataset.csv.\n",
      "Missing LDC countries saved to ./audit_data/m_and_a_missing_ldcs.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "m_and_a_data = pd.read_csv(\"./raw_data/m_and_a_data_2017_2018.csv\")\n",
    "name_conversions = pd.read_csv(\"./audit_data/m_and_a_name_conversions.csv\")\n",
    "ldcs_list = pd.read_csv(\"./variable_data/ldcs_list_2017_2018.csv\")\n",
    "\n",
    "\n",
    "# Function to standardize country names and convert to snake_case\n",
    "def standardize_country_names(data_df, conversion_df):\n",
    "    # Apply name mapping\n",
    "    name_map = conversion_df.set_index(\"old_name\")[\"new_name\"].dropna().to_dict()\n",
    "    data_df[\"destination_country\"] = data_df[\"TNATION\"].replace(name_map)\n",
    "\n",
    "    # Then convert the names to snake_case\n",
    "    data_df[\"destination_country\"] = (\n",
    "        data_df[\"destination_country\"]\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\", regex=False)\n",
    "        .str.replace(\"-\", \"_\", regex=False)\n",
    "        .str.replace(\",\", \"_\", regex=False)\n",
    "    )\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "# Apply the name conversion function to M&A dataframe\n",
    "m_and_a_data = standardize_country_names(m_and_a_data, name_conversions)\n",
    "\n",
    "# Filter out all entries for which the target nation is not an LDC\n",
    "filtered_m_and_a_data = m_and_a_data[\n",
    "    m_and_a_data[\"destination_country\"].isin(ldcs_list[\"country\"])\n",
    "]\n",
    "\n",
    "# Identify LDC countries that do not appear in the M&A dataset\n",
    "ldc_countries_set = set(ldcs_list[\"country\"])\n",
    "destination_countries_set = set(\n",
    "    filtered_m_and_a_data[\"destination_country\"].dropna().unique()\n",
    ")\n",
    "missing_countries = sorted(ldc_countries_set - destination_countries_set)\n",
    "\n",
    "# Print missing countries\n",
    "print(\"LDC countries not appearing in the M&A dataset's destination_country column:\")\n",
    "for country in missing_countries:\n",
    "    print(country)\n",
    "\n",
    "# Save the list of missing countries to a CSV file\n",
    "missing_countries_filepath = \"./audit_data/m_and_a_missing_ldcs.csv\"\n",
    "missing_countries_df = pd.DataFrame(missing_countries, columns=[\"missing_countries\"])\n",
    "missing_countries_df.to_csv(missing_countries_filepath, index=False)\n",
    "\n",
    "# Save the filtered data\n",
    "output_filepath = \"./processed_data/m_and_a_main_dataset.csv\"\n",
    "filtered_m_and_a_data.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to {output_filepath}.\")\n",
    "print(f\"Missing LDC countries saved to {missing_countries_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of unique countries appearing in \"destination_country\" and counts, as well as total size of filtered dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'destination_country:\n",
      "destination_country\n",
      "afghanistan                      3\n",
      "angola                          15\n",
      "bangladesh                      23\n",
      "benin                            5\n",
      "bhutan                           1\n",
      "burkina_faso                    14\n",
      "burundi                          3\n",
      "cambodia                        50\n",
      "central_african_republic         2\n",
      "chad                             4\n",
      "democratic_republic_of_congo     9\n",
      "djibouti                         4\n",
      "east_timor                       2\n",
      "eritrea                          1\n",
      "ethiopia                        13\n",
      "gambia                           1\n",
      "guinea                           2\n",
      "haiti                            4\n",
      "laos                            13\n",
      "lesotho                          7\n",
      "liberia                          2\n",
      "madagascar                      10\n",
      "malawi                           7\n",
      "mali                            10\n",
      "mauritania                       4\n",
      "mozambique                      13\n",
      "myanmar                         45\n",
      "nepal                           16\n",
      "niger                            3\n",
      "rwanda                           7\n",
      "senegal                         16\n",
      "sierra_leone                     2\n",
      "solomon_islands                  2\n",
      "somalia                          1\n",
      "sudan                            4\n",
      "tanzania                        26\n",
      "togo                             8\n",
      "uganda                          18\n",
      "vanuatu                          3\n",
      "yemen                            2\n",
      "zambia                          31\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of observations in filtered data: 406\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "m_and_a_dataset = pd.read_csv(\"./processed_data/m_and_a_main_dataset.csv\")\n",
    "\n",
    "# Get unique values appearing in \"destination_country\" and their counts\n",
    "destination_counts = m_and_a_dataset[\"destination_country\"].value_counts().sort_index()\n",
    "\n",
    "print(\"Unique values in 'destination_country:\")\n",
    "print(destination_counts)\n",
    "\n",
    "# Get the total number of observations\n",
    "total_observations = len(m_and_a_dataset)\n",
    "print(\"\\nTotal number of observations in filtered data:\", total_observations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset has been filtered down to just those transactions with LDCs as destination countries. Next step is to get ultimate parent country for all entities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
